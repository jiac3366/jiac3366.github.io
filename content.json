{"meta":{"title":"Hexo","subtitle":"","description":"","author":"Jiaccc","url":"https://jiac3366.github.io","root":"/"},"pages":[{"title":"友情链接","date":"2021-09-15T13:19:16.012Z","updated":"2021-09-15T13:19:16.012Z","comments":true,"path":"links/index.html","permalink":"https://jiac3366.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-09-15T13:19:16.013Z","updated":"2021-09-15T13:19:16.013Z","comments":false,"path":"tags/index.html","permalink":"https://jiac3366.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2021-09-15T13:19:16.011Z","updated":"2021-09-15T13:19:16.011Z","comments":false,"path":"categories/index.html","permalink":"https://jiac3366.github.io/categories/index.html","excerpt":"","text":""},{"title":"404 Not Found：该页无法显示","date":"2021-09-15T14:01:41.464Z","updated":"2021-09-15T13:19:16.010Z","comments":false,"path":"/404.html","permalink":"https://jiac3366.github.io/404.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-09-15T13:19:16.012Z","updated":"2021-09-15T13:19:16.012Z","comments":false,"path":"repository/index.html","permalink":"https://jiac3366.github.io/repository/index.html","excerpt":"","text":""},{"title":"关于","date":"2021-09-15T13:19:16.011Z","updated":"2021-09-15T13:19:16.011Z","comments":false,"path":"about/index.html","permalink":"https://jiac3366.github.io/about/index.html","excerpt":"","text":"个人详细介绍"}],"posts":[{"title":"","slug":"Untitled","date":"2021-12-19T16:37:29.933Z","updated":"2021-12-19T16:37:29.933Z","comments":true,"path":"2021/12/20/Untitled/","link":"","permalink":"https://jiac3366.github.io/2021/12/20/Untitled/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"网络 | HTTPS解决了什么问题","slug":"计算机网络/HTTPS","date":"2021-12-19T16:37:19.227Z","updated":"2021-12-19T16:40:25.980Z","comments":true,"path":"2021/12/20/计算机网络/HTTPS/","link":"","permalink":"https://jiac3366.github.io/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTPS/","excerpt":"","text":"HTTPS解决的问题 TLS1.0就是SSL v3.1​对于小文件 握手会更多影响性能 主要考验非对称加密算法eg: RSA性能对于大文件 对称加密算法eg: AES更影响 混合加密——机密性只要公钥才能解开私钥加密的信息 摘要算法——完整性 会话密钥加密：明文摘要，会话密钥接触明文和摘要，重新用摘要算法加密明文和收到的摘要对比 数字证书——身份验证 ：你怎么鉴别别人给你的公钥是对的？毕竟每个人都可以创建自己的公钥和私钥 客户端先向服务器端索要的不是服务器的公钥，而是经过CA验证的数字证书许多浏览器和操作系统内置根证书，可以人为相信一个证书 数字证书包含但不限于服务器的公钥（相当于原文）和数字签名（CA私钥加密服务器公钥的摘要，私钥加密摘要就是签名） 用CA公钥验证数字证书——客户端使用CA的公钥解密数字签名得到摘要1，再用摘要算法加密服务器公钥，结果与摘要1作对比（验证CA身份和公钥完整性） HTTPS如何通信 需要2对公钥私钥因为黑客可以使用一方的公钥模拟另一方 HTTPS通信大致流程 客户端验证数字证书 协商生成会话密钥 使用非对称加密传输会话密钥 双方采用会话密钥进行通信 TLS1.2 的四次握手大致流程 (TLS1.3是三次握手-p41) clientHello tls协议版本 随机数（协商对称密钥的时候使用）Nonce 随机数保证唯一，或者 Timestamp 和 Nonce 合起来保证唯一，同样的，请求只接受一次，于是服务器多次收到相同的 Timestamp 和 Nonce，则视为无效即可 密码套件、压缩算法 serverHello 确认clientHello内容：协议版本和密码套件、压缩算法 并返回随机数、数字证书 client继续做下列事情： 通过浏览器或者OS中的 CA 公钥去解密server的数字证书，如果双端tls，也把自己的证书发过去 如果证书ok：取出服务器公钥加密发送以下内容 Client Key Exchange：生成并发送第三个随机数Pre-Master到目前为止，客户端和服务器都有了三个随机数，分别是：自己的、对端的，以及刚生成的 Pre-Master 随机数。通过这三个随机数，可以在客户端和服务器产生相同的对称密钥（不发送）。 Change Cipher Spec: 表示接下来用会话密钥加密： Encrypted Handshake Message： 把以前的内容都做个摘要供server校验 server回应：通过3个随机数和协商的加密套件，计算本次的会话密钥（各⾃⽣成，如果前面3个随机数一致，应该大家生成的都一样） 同样也发送 Change Cipher Spec和 Encrypted Handshake Message 的消息 openssl命令 generated key-cert ： openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj “/CN=cncamp.com/O=cncamp“ -addext “subjectAltName = DNS:cncamp.com“自己给自己颁发证书,Self-Signed Certificate 通过这个命令先创建私钥 openssl genrsa -out cliu8siteprivate.key 1024 根据这个私钥，创建对应的公钥 openssl rsa -in cliu8siteprivate.key -pubout -outcliu8sitepublic.pem 生成证书需要发起一个证书请求，然后将这个请求发给CA去认证： openssl req -key cliu8siteprivate.key -new -out cliu8sitecertificate.reqCA会给这个证书卡一个章，我们称为签名算法。签名算法大概是这样工作的：一般是对信息做一个 Hash 计算，得到一个 Hash 值。把信息发送出去时，把这个 Hash 值加密后，作为一个签名,和信息一起发出去。​ 权威机构给证书签名的命令是这样的: openssl x509 -req -in cliu8sitecertificate.req -CA cacertificate.pem -CAkey caprivate.key -out cliu8sitecertificate.pem这个命令会返回 Signature ok，而 cliu8sitecertificate.pem 就是签过名的证书。CA 用自己的私钥给外卖网站的公钥签名，就相当于给外卖网站背书，形成了外卖网站的证书。​ 查看这个证书的内容:openssl x509 -in cliu8sitecertificate.pem -noout -text这里面有个 Issuer，也即证书是谁颁发的；Subject，就是证书颁发给谁；Validity 是证书期限；Public-key 是公钥内容；Signature Algorithm 是签名算法","categories":[{"name":"网络","slug":"网络","permalink":"https://jiac3366.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"数据库 | 数据库设计","slug":"数据库/数据库设计","date":"2021-12-19T16:31:48.151Z","updated":"2021-12-19T16:32:26.220Z","comments":true,"path":"2021/12/20/数据库/数据库设计/","link":"","permalink":"https://jiac3366.github.io/2021/12/20/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"SQL范式设计——围绕非主属性/主属性是否对于候选键是直接依赖？ 码 = 候选键(唯一确定记录的键)= 所有主键——其包含的所有属性都叫主属性2NF：消除了非主属性对于候选键的部分依赖。3NF：消除了非主属性对于候选键的传递依赖。BCNF：消除主属性对于候选键的部分与传递依赖。 1NF：字段不可拆分，任何的 DBMS 都会满足第一范式 2NF：表中非主属性要和表的候选键有完全依赖关系即两个对象不能掺和在一起，2NF 告诉我们一张表就是一个独立的对象 3NF：任何非主属性都不传递依赖于候选键——保证只能由候选键直接决定非主属性，非主属性不能决定非主属性不能存在非主属性 A 依赖于非主属性 B，非主属性 B 依赖于候选键的情况。 eg：现在有一张学生选课表，包含的属性有学号、姓名、课程名称、分数、系别和系主任 学生和课程要分开 –&gt;不是同一个对象，违反2NF分成Student(学号,姓名,系别)和Course(课程号，课程名称)2个对象中间加个表关联: SC(学号,课程号,分数) 要分成Student(学号,姓名,系别)和Sdept(系别,系主任)，因为学生决定系别，系别又决定系主任–&gt;违反3NF非主属性系主任会传递依赖于学号 BCNF：在 3NF 的基础上消除了主属性对候选键的部分依赖或者传递依赖关系。第22章 反范式设计：通过空间换时间，提升查询的效率 在数据量大情况下 直接在评论表中加入用户名字段 就不用查询2个表 使用场景：冗余信息有价值或者能大幅度提高查询效率的时候，数据仓库在设计上更偏向采用反范式设计。如订单中的收货人信息，包括姓名、电话和地址等需要保存记录，但用户可以轻易修改这些信息 缺点：1.让数据库的设计更加复杂。2.增加系统的维护成本。比如用户每次更改昵称的时候，都需要执行存储过程来更新，如果昵称更改频繁，会非常消耗系统资源 杂谈：候选键就像这一行数据的老大，它能唯一决定这行记录。当然，这条记录有可能有多个老大，候选键中不包含的属性就是小弟们，2NF规定，老大与小弟们的关系，要么1对多，要么多对1，否则就不符合2NF。 ER图设计 超市的进货模块：先确定实体，再拓展属性 供货商、门店和顾客强实体，仓库和员工弱实体(虽然都可以独立存在，但是弱实体依赖门店这个实体)独立存在的是实体，不可再分的是属性 销售模块 商品和门店不依赖于任何其他实体，所以是强实体；会员和收银款台都依赖于门店，所以是弱实体 如何从ER图转换至数据表： （上图P和Q表示多，只是为了与M、N区别） 一个实体通常转换成一个数据表，弱实体中加入强实体的外键，强实体和弱实体就是One2manyeg: 仓库转换成仓库表（demo.stock）–branchid INT NOT NULL​– 设置外键约束，与门店表（demo.branch）关联-&gt; CONSTRAINT fk_stock_branch FOREIGN KEY (branchid) REFERENCES branch (branchid)-&gt; ); 一个多对多的关系，通常也转换成一个数据表（一般就是一些流水表，得取决业务涉及到什么实体）这个 ER 模型中的多对多的关系有 2 个，分别是销售关系和进货关系eg：进货关系表关联到 4 个实体，分别是供货商、商品、仓库、员工，所以，表中必须要包括这 4 个实体转换成的表的主键，按照数据表设计的第三范式的要求和业务优先的原则，我们把这个进货单表拆分成 2 个表，分别是进货单头表和进货单明细表（odoo就是这么干的） 一个 1 对 1，或者 1 对多的关系，往往通过表的外键来表达，而不是设计一个新的数据表eg：在流水单头表中，分别把 cashierid、memberid 和 operatorid 定义成了外键 分库分表 需要设计阶段就完成，类中可以使用正则去计算访问的服务器、数据库和表名称？？？ 垂直分表——拆分商品表 常用字段表和不常用字段表 垂直分库 按照模块功能分库 销售、库存、营运、会员 优点：数据量和访问流量都减少，不同业务之间的数据交互减少，故障风险减小 水平分表 拆分已处理和正在处理的数据（DML频繁的数据）已经验收的表，增加“验收”相关的字段 例如验收人和验收日期,经过验收的表不经常修改，只负责查询 拆分时间太久远的数据到历史表 也可以根据业务评估拆分，确保单个表数据量适中例如对id大于500小于1000的数据拆到其他表 水平分库 和水平分表类似，例如每500个数据（商户），在同一个服务器创建新的数据库；每5000个数据，放置在不同的数据库 问题：","categories":[{"name":"数据库","slug":"数据库","permalink":"https://jiac3366.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[]},{"title":"Python高级 | 多进程多线程","slug":"Python高级/多进程多线程","date":"2021-12-19T16:22:03.896Z","updated":"2021-12-19T16:29:52.707Z","comments":true,"path":"2021/12/20/Python高级/多进程多线程/","link":"","permalink":"https://jiac3366.github.io/2021/12/20/Python%E9%AB%98%E7%BA%A7/%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"多进程多线程 threading run()触发你传入的函数 协程 当I/O 操作非常多、非常 heavy、需要建立的连接也比较多的时候，一般会创建挺多的线程，但线程池线程数一般有一个建议数值。但是协程对于这种场景可以创建很多的协程数量来应对，防止线程池不够用的场景。 GIL 为什么有 GIL？和 CPython 的实现有关 GIL，CPython 解释器 中1个技术术语 1、防止使用多个线程被分配到多个CPU执行用一份代码(?造成内存管理的引用计数发生race conditionCPython 使用引用计数来管理内存，如果有两个 Python 线程同时引用了 a，就会造成引用计数的 race condition，引用计数可能最终只增加 1，这样就会造成内存被污染。因为第一个线程结束时，会把引用计数减少 1，这时可能达到条件释放内存，当第二个线程再试图访问 a 时，就找不到有效的内存了。 2、CPython 大量使用 C 语言库，但大部分 C 语言库都不是原生线程安全的（线程安全会降低性能和增加复杂度）。 GIL如何工作： 每一个线程在开始执行时，都会锁住 GIL，以阻止别的线程执行 CPython 中有个check_interval机制 ，去轮询检查线程 GIL 的锁住情况，在一个“合理”的时间范围内释放 GIL每隔一段时间，Python 解释器就会强制当前线程去释放 GIL，这样别的线程才能有执行的机会 所以check_interval机制导致随时打断程序运行，有了 GIL也还要继续考虑线程安全 总结：多线程也可以被调度到多个CPU上，只是由于python的GIL的存在，python的单线程和多线程同时都只能利用一颗cpu核心，对于纯cpu heavy任务场景，不涉及到io耗时环节，cpu都是充分利用的，多线程和单线程相比反倒是多了线程切换的成本，所以性能反而不如单线程。 Python垃圾回收机制 引用计数 对象的引用计数（指针数）为 0 的时候，自然它也就成为了垃圾，需要被回收 循环引用问题（有两个 list 互相引用，这里极有可能引起内存泄露） 引用计数为0不是垃圾回收的充要条件 Python 使用标记清除（mark-sweep）算法和分代收集（generational）启用针对循环引用的自动垃圾回收 标记清除（mark-sweep）算法 相当于遍历全图，在遍历结束后，所有没有被标记的节点，我们就称之为不可达节点。但图遍历性能消耗严重，mark-sweep 使用双向链表维护了一个数据结构，并且只考虑容器类的对象，只有容器类对象才有可能产生循环引用 分代收集（generational）算法‘’ 新生的对象更有可能被垃圾回收，而存活更久的对象也有更高的概率继续存活 objgraph调试内存泄漏，可视化引用关系 show_refs()，它可以生成清晰的引用关系图 show_backrefs() Python Object Graphs — objgraph 3.5.0 documentation (pov.lt)","categories":[{"name":"Python高级","slug":"Python高级","permalink":"https://jiac3366.github.io/categories/Python%E9%AB%98%E7%BA%A7/"}],"tags":[]},{"title":"Python高级 | Spider Info","slug":"Python高级/爬虫资料汇总","date":"2021-12-19T16:19:45.280Z","updated":"2021-12-19T16:23:28.765Z","comments":true,"path":"2021/12/20/Python高级/爬虫资料汇总/","link":"","permalink":"https://jiac3366.github.io/2021/12/20/Python%E9%AB%98%E7%BA%A7/%E7%88%AC%E8%99%AB%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB/","excerpt":"","text":"Cookie登录 Cookie池 https://github.com/Python3WebSpider/CookiesPool 获取方式： Request解密js https://github.com/pig6/login_taobao https://github.com/CharlesPikachu/DecryptLogin Selenium模拟 隐藏头部 未闻Code 鼠标滑动 Welcome to PyAutoGUI’s documentation! — PyAutoGUI documentation 代理IP ASDL https://github.com/Python3WebSpider/AdslProxy https://blog.csdn.net/u014094101/article/details/111192564?spm=1001.2014.3001.5501 代理池 https://github.com/Python3WebSpider/ProxyPool 验证码 极验 https://github.com/Python3WebSpider/CrackGeetest 易盾 https://github.com/yujunjiex/behavior_captcha_cracker 逆向 Germey/Review_Reverse: 2019年末总结下今年做过的逆向，整理代码，复习思路。 拼夕夕Web端anti_content参数逆向分析 WEB淘宝sign逆向分析； 努比亚Cookie生成逆向分析； 百度指数data加密逆向分析 今日头条WEB端_signature、as、cp参数逆向分析 知乎登录formdata加密逆向分析 KNN猫眼字体反爬 Boss直聘Cookie加密字段__zp_stoken__逆向分析 (github.com) More: Python3WebSpider、https://github.com/Germey 崔庆才https://github.com/asyncins 韦世东 https://github.com/Python3WebSpider/TaobaoProduct 未闻Code","categories":[{"name":"Python高级","slug":"Python高级","permalink":"https://jiac3366.github.io/categories/Python%E9%AB%98%E7%BA%A7/"}],"tags":[]},{"title":"Golang |  类型转换","slug":"Golang/Go程序实体","date":"2021-12-19T16:14:12.484Z","updated":"2021-12-19T16:16:49.354Z","comments":true,"path":"2021/12/20/Golang/Go程序实体/","link":"","permalink":"https://jiac3366.github.io/2021/12/20/Golang/Go%E7%A8%8B%E5%BA%8F%E5%AE%9E%E4%BD%93/","excerpt":"","text":"“类型断言”表达式 value, ok := interface{}(container).([]string) ——理解为python的isinstance()还有额外转换赋值功能 interface{}(container) 把container变量的值转换为空接口值，因为类型断言表达式的语法形式是x.(T)。其中的x必须是接口类型的，例如：.([]string) 判断前者的类型是否为切片类型 []string，如果是true，那么被判断的值将会被自动转换为[]string类型的值，并赋给变量value，否则value将被赋予nil（即“空”） 区分对比：类型转换表达式的语法形式是T(x)x可以是一个变量，也可以是一个代表值的字面量（比如1.23和struct{}），还可以是一个表达式 一对不包裹任何东西的花括号，除了可以代表空的代码块之外，还可以用于表示不包含任何内容的数据结构（数据类型），如：struct{}、interface{}、[]string{}、map[int]string{}，而类型字面量是用来表示数据类型本身的若干字符，如：interface、map[int]string、[]string GO的一些陷阱： 1、uint8和int16之间的转换?? int16类型的值-255的补码是1111111100000001。如果我们把该值转换为int8类型的值，那么 Go 语言会把在较高位置（或者说最左边位置）上的 8 位二进制数直接截掉，从而得到00000001。所以var srcInt = int16(-255)dstInt := int8(srcInt) ——dstInt的值就是1 2、当把一个浮点数类型的值转换为整数类型值时，前者的小数部分会被全部截掉 3、字符’�’是 Unicode 标准中定义的 Replacement Character，专用于替换那些未知的、不被认可的以及无法展示的字符，由于-1肯定无法代表一个有效的 Unicode代码点， string(-1)所以得到的总会是”�” 别名类型和潜在类型 type MyString = string，别名类型主要是为了代码重构而存在的，byte是uint8的别名类型，而rune是int32的别名类型，没有其他区别 type MyString2 string，MyString2是一个全新的类型，潜在类型相同的不同类型的值之间是可以进行类型转换的 string可以被称为MyString2的潜在类型，表示MyString2 在本质上是string MyString2类型的值与string类型的值可以使用T(x)表达式进行互转 但对于集合类的类型[]MyString2与[]string来说这样做却是不合法的，因为[]MyString2与[]string的潜在类型不同，分别是[]MyString2和[]string","categories":[{"name":"Golang","slug":"Golang","permalink":"https://jiac3366.github.io/categories/Golang/"}],"tags":[]},{"title":"Golang | Golang的包管理","slug":"Golang/Golang包管理机制","date":"2021-12-19T16:09:36.179Z","updated":"2021-12-19T16:14:06.347Z","comments":true,"path":"2021/12/20/Golang/Golang包管理机制/","link":"","permalink":"https://jiac3366.github.io/2021/12/20/Golang/Golang%E5%8C%85%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6/","excerpt":"","text":"01 ** | **GOPATH 有什么意义 GOPATH 每个目录都代表 Go 语言的一个工作区（workspace） Go 语言项目在其生命周期内的所有操作（编码、依赖管理、构建、测试、安装等）基本上都是围绕着 GOPATH 和工作区进行 Go 语言源码的组织方式 一个代码包的导入路径实际上就是从 src 子目录，到该包的实际存储位置的相对路径 了解源码安装后的结果 安装后如果产生了可执行文件，就可能会放进该工作区的 bin 子目录 安装后如果产生了归档文件（执行go install 后的.a文件，程序编译后生成的静态库文件/静态链接文件），就会放进该工作区的 pkg子目录 生成的归档文件放置它的目录是：代码包的导入路径的直接父级 + pkg的平台相关目录 + 代码包目录 理解构建和安装 Go 程序的过程 build和install都会执行编译、打包等操作 go build 不能生成包文件, go install 可以生成包文件 构建 go build (主要意义在于测试) 构建的是库源码文件，结果文件只会存在临时目录。 构建的是命令源码文件，结果文件会在源码文件所在目录 安装 go install （库源码文件?命令源码文件?） 操作会先执行构建，然后还会进行链接操作，把结果文件搬运到指定目录 安装的是库源码文件，结果文件会被搬运到它所在工作区的 pkg 目录下的某个子目录中 安装的是命令源码文件，结果文件会被搬运到它所在工作区的 bin 目录中，或者环境变量GOBIN指向的目录中 go get 相关 (20条消息) go get命令_benben的博客-CSDN博客 1、对代码包的远程导入路径进行自定义的方法是：在该代码包中的库源码文件的包声明语句的右边加入导入注释，例如：package semaphore // import “golang.org/x/sync/semaphore“， 2、而加入导入注释之后，用以下命令即可下载并安装该代码包go get golang.org/x/sync/semaphore，Go 语言官网 golang.org 下的路径 /x/sync/semaphore 并不是存放semaphore包的真实地址。我们称之为代码包的自定义导入路径 02丨命令源码文件 命令源码文件是程序的运行入口，只能有一个，如果一个源码文件声明属于main包，并且包含一个无参数声明且无结果声明的main函数，那么它就是命令源码文件通过构建或安装命令源码文件，生成的可执行文件就可以被视为“命令”，既然是命令，那么就应该具备接收参数的能力。 如果目录中有命令源码文件，那么其他种类的源码文件（同目录下的其他文件？）也应该声明属于main包 flag包 在运行命令源码文件的时候传入参数，查看参数的使用说明 自定义命令源码文件的参数使用说明 场景：更灵活地定制命令参数容器，可以用 Go 编写命令，并可以让它们像众多操作系统命令那样被使用，甚至可以把它们嵌入到各种脚本中。 在调用flag包中的一些函数（比如StringVar、Parse等等）的时候，实际上是在调用flag.CommandLine变量的对应方法。 实践：自己创建一个私有的命令参数容器？ 03丨库源码文件 库源码文件是不能被直接运行的源码文件，它仅用于存放程序实体，这些程序可以被其他代码使用 go build jiac_gateway/try03 –路径是相对于GOPATH的src的，同属于main包，1_lib里的函数不用大写也可以被1.go引用 库源码文件 1_lib.go 所在目录的相对路径是jiac_gateway/try_03_split/lib，而它却声明自己属于library包，如何导入？ 下面是需要弄清楚的，但写法不提倡，我们应该让声明的包名与其父目录的名称一致 1_lib.go首字母为大写的程序实体才可以被当前包外的代码引用 go除了包权限规则，还可以通过创建internal代码包让代码仅仅能被当前模块中的其他代码引用（模块级私有） 课后： 如导入两个代码包，而这两个代码包的导入路径的最后一级是相同的，比如：dep/lib/flag和flag，怎样解决这种冲突，有几种方式？ A：导入代码包的时候给它起一个别名就可以了，比如： import libflag “dep/lib/flag”。或者，以本地化的方式???导入代码包，如：import . “dep/lib/flag”（”dep/lib/flag”包中公开的程序实体，会被视为当前代码包中的程序实体，eg: 导入语句import . fmt，直接用Printf就可以）","categories":[{"name":"Golang","slug":"Golang","permalink":"https://jiac3366.github.io/categories/Golang/"}],"tags":[]},{"title":"后端架构 | 负载均衡策略有哪些？","slug":"后端架构/负载均衡策略","date":"2021-12-19T16:03:33.043Z","updated":"2021-12-19T16:05:59.969Z","comments":true,"path":"2021/12/20/后端架构/负载均衡策略/","link":"","permalink":"https://jiac3366.github.io/2021/12/20/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AD%96%E7%95%A5/","excerpt":"","text":"SLB linux负载均衡总结性说明（四层负载/七层负载） - 散尽浮华 - 博客园 (cnblogs.com)linux负载均衡总结性说明（四层负载/七层负载） - 散尽浮华 - 博客园 (cnblogs.com) HTTP 重定向负载均衡（较少用）应用服务器不得不使用公网 IP，外部访问者可以直接连接到应用服务器，系统的安全性会降低 DNS 负载均衡 两次负载均衡，一次通过 DNS 负载均衡，用户请求访问数据中心负载均衡服务器集群的某台机器，然后这台负载均衡服务器再进行一次负载均衡，将用户请求分发到应用服务器集群的某台服务器上. 不同的用户进行域名解析的时候，返回不同的 IP（数据中心负载均衡服务器的 IP ）第一次解析后，域名对应的IP缓存在本机，性能比HTTP重定向好 默认是轮询算法 缺点是TTL 维护性差：客户端在TTL周期内永远用那一个IP，到过期了才采用下一个IP，这期间服务端IP变了客户端无法得知 扩展性低：控制权在ISP 可用性差：解析时间长 反向代理负载均衡（应用层（七层）负载均衡——Nginx/ Ingress/ Envoy是数据面 Istio是控制面） 反向代理服务器是工作在 HTTP 协议层之上的，代理的也是 HTTP 的请求和响应，效率比较低反向代理负载均衡通常用在小规模的互联网系统上，只有几台或者十几台服务器的规模。 GRPC LB是能做的，TCP转包是能做的，但如果是单客户端，有一个客户端流量很大，因为复用TCP的原因，如果有单个客户端发很大请求，其他客户端发很少的请求，大量的请求会到一个服务器去，不能保证均衡。一般需要应用层LB器支持。所以kube-proxy没办法满足这些保持连接的服务，一般通过nginx和envoy支持。还有一个问题，安全问题：service管不了应用层的东西，比如证书，所以ingress就是解决这些问题 隧道技术——加一层包头(CNI是这么做的) VLAN 加UDP包头 IPinIP LB封多一层包，服务器解包，然后从旁路回去 TCP负载均衡 有应用跑在LB设备上，客户端与LB设备、LB设备与实际服务器都建立TCP连接 如何把客户端IP往下带（虽然IP也会变化，但是用部分技术可以做到） TOC 协议 ，TCP包头有option位，可以把客户端IP支持，但要kernel都支持 proxy protocol，预留了TCP数据包第一个部分可以存客户端IP，需要上下游应用都支持（Nginx支持） IP 负载均衡（ 网络层（三层）负载均衡, 有流量瓶颈） 在操作系统内核直接修改 IP 数据包的地址，效率高 缺点是所有请求都要通过负载均衡服务器进行 IP 地址转换，况且响应的数据很大时会成为响应数据的流量瓶颈 NAT(不鼓励) 修改目标 IP和源 IP(LB设备实际IP) ,客户访问的LB设备IP是虚拟IP，LB设备维护一个新目标IP(实际服务器)到客户端连接的记录表 缺点：客户端IP丢失 数据链路层负载均衡 （应用服务器和负载均衡服务器都使用相同的虚拟 IP地址） ——DR模式到了pod 解得剩下里层包， 回包直接走DR模式通过默认网关回到client，回去直接路由 目前大型互联网应用基本都使用链路层负载均衡，虚拟 IP地址是什么原理？ 不常见 负载均衡服务器并不修改数据包的 IP 地址，响应无需经过负载均衡服务器，解决响应数据量大,输出带宽不足的问题 负载均衡服务器的地位和路由器下的交换机类似，修改数据包的MAC地址，响应会直接到达用户的浏览器，而不会再经过负载均衡服务器貌似决定负载均衡服务器交给哪台应用服务器去处理请求，是由用户的 mac 地址决定的 缺点：客户端和负载均衡服务器和应用服务器都要在同一个二层，外部客户端不支持 TIPS: Linux 上实现 IP 负载均衡和链路层负载均衡的技术是 LVS(ipvs)，目前 LVS 的功能已经集成到Linux 中了，通过 Linux 可以直接配置实现这两种负载均衡 负载均衡算法有轮询、随机、最少连接 写一个简化的 HTTP 重定向负载均衡 demo","categories":[{"name":"后端架构","slug":"后端架构","permalink":"https://jiac3366.github.io/categories/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84/"}],"tags":[]},{"title":"后端架构 | 架构知识浅谈","slug":"后端架构/架构知识","date":"2021-11-17T00:01:56.868Z","updated":"2021-12-19T16:17:24.604Z","comments":true,"path":"2021/11/17/后端架构/架构知识/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84/%E6%9E%B6%E6%9E%84%E7%9F%A5%E8%AF%86/","excerpt":"","text":"Simple evolution 第一次分离的时候，应用程序、数据库、文件系统分别部署在不同的服务器上 使用缓存改善性能，通过缓存读取数据。缓存主要有分布式缓存和本地缓存两种。分布式缓存将多台服务器共同构成一个集群，存储更多的缓存数据 因为连接大量的并发用户的访问，通过负载均衡服务器，将应用服务器部署为一个集群 数据库的读写分离 大多数的互联网应用而言，以上的分布式架构就已经可以满足用户的并发访问压力了 更上一层还需要解决： 【海量数据的存储与查询】，主要通过分布式数据库、分布式文件系统、NoSQL 数据库解决 【网络带宽压力以及访问延迟】？部署独立的搜索引擎提供查询服务。同时减少数据中心的网络带宽压力，提供更好的用户访问延时、使用 CDN 和反向代理提供前置缓存 【实现系统的低耦合与模块化开发和部署】为了使各个子系统更灵活易于扩展，使用分布式消息队列将相关子系统解耦，通过消息的发布订阅完成子系统间的协作、使用微服务架构将逻辑上独立的模块在物理上也独立部署，单独维护，应用系统通过组合多个微服务完成自己的业务逻辑，实现模块更高级别的复用从而更快速地开发系统和维护系统 Cache 减少 CPU 的计算消耗，节省计算资源​通读缓存（read-through）旁路缓存（cache-aside）区别在于缓存是否负责帮应用程序从数据源读取数据 通读缓存（read-through） 如果没有，通读缓存就自己负责访问数据源，从数据源获取数据返回给应用程序，并将这个数据缓存在自己的缓存中，通常会作为系统架构的一部分，很多时候对应用程序是透明的 CDN静态内容和动态内容部署在不同的服务器集群上，使用不同的二级域名，即所谓的动静分离 反向代理缓存 设计HTTP代理缓存 旁路缓存（cache-aside） 如果没有，就返回空（null） 对象缓存 本地缓存，使用和应用程序在同一个进程的堆空间存放缓存数据 分布式缓存 （每个程序需要依赖一个Memcached 的客户端 SDK） 应用程序调用 API，API 调用 SDK 的路由算法，路由算法根据缓存的 key 值，计算这个key对应的内容的服务器 IP 地址和端口号，API 再调用 SDK 的通信模块，将 &lt;key, value&gt; 值以及缓存操作命令发送给具体的某台服务器 解决数据脏读问题 过期失效（使用更多） 失效通知 路由hash算法遇到增加服务器时候，会造成大量缓存不命中，可以用一致性哈希算法解决？？？ Asynchronous architecture（Event driven architecture） 痛点：如何提高系统的写操作的性能呢？两个应用系统之间需要互相调用，其实把两个应用耦合起来了，被调用的应用产生了故障或者升级，都可能会引起调用者故障，或者也不得不升级！ 消息队列的职责就是缓冲消息，等待消费者消费（在2个相互调用的服务之间增加一个队列）。根据消息消费方式又分为点对点模式和发布订阅模式两种。 点对点模式（保证服务的） 消费者程序可以部署在多台服务器上，但是对于任何一个消息，只会被发送给其中的一个消费者服务器。 这些服务器可以根据消息的数量动态伸缩，保证邮件能及时发送。 如果有某台消费者服务器宕机，既不会影响其他消费者处理消息发送邮件，也不会影响生产者程序正常运行 发布订阅模式 在消息队列中设置主题，多个消息消费者可以订阅同一个主题，每个消费者都可以收到这个主题的消息拷贝 与点对点区别：消息生产者不需要自行构造不同的业务消息到对应的mq中，只需要把普通数据加入到mq的某个主题，订阅该主题的不同消费者根据自己的业务消费该数据eg 新用户注册的时候一方面需要发送激活邮件，另一方面可能还需要发送欢迎短信，还可能需要将用户信息同步给关联产品或数据库 该架构好处： 改善写操作请求的响应时间 更容易进行伸缩 负载均衡实现集群伸缩，但是这种集群伸缩是以整个应用服务器为单位的，但如果只是某些功能（例如图像处理）有压力，使用mq单独针对图片处理的消费者集群进行伸缩 削峰填谷 消费者可以控制消费速度，降低系统访问高峰时压力，在访问低谷时继续消费消息队列中未处理的消息，保持系统的资源利用率 隔离失败 消费者如果在处理消息的过程中失败，不会传递给生产者 降低耦合 耦合会使软件僵硬、笨拙、难以维护 代码的依赖 返回调用结果的依赖如果调用出现异常，应用程序必须要处理这个异常 Data storage architecture 改善数据存储能力的主要手段包括：数据库主从复制、数据库分片、业务分库和NoSQL 数据库 主从复制（提高可用性，无法提升存储能力） 一主多从有的从数据库用来做实时数据分析，有的从数据库用来做批任务报表计算，有的单纯做数据备份 两主多从 两台服务器互相备份，仅仅用来提升数据写操作的可用性，并不能用来提高写操作的性能 所有的应用程序都必须连接到同一个主数据库进行写操作，只有当该数据库宕机失效的时候，才会将写操作切换到另一台主数据库上。 业务分库（提高存储能力） 将不同业务相关的数据库表，部署在不同的服务器上，每一类数据库还可以继续选择使用主从复制，或者主主复制 数据库分片（提高存储能力） 硬编码方式分片（根据数据ID映射成服务器编号），缺点是增加节点数，逻辑要修改 分布式关系数据库中间件分片（例如MYCAT）–类似查表法 MYCAT 就可以解析出 SQL 中的地区字段 prov（根据地区进行数据分片），根据这个字段连接相对应的数据库 余数 Hash 算法分片（更常见，分布均匀） 根据主键 ID 和服务器的数目进行取模计算，根据余数连接相对应的服务器 一致性hash算法 NoSQL 数据库（Key、Value 的方式进行数据访问） 常用的 NoSQL 数据有 Apache HBase，Apache Cassandra、Redis， 与RDMS主要区别可用RDMS的ACID和NoSQL的BASE概括 CAP 原理 一个提供数据服务的分布式系统无法同时满足数据一致性（Consistency）、可用性（Availability）和分区耐受性（Partition Tolerance）这三个条件。 一个分布式系统而言，网络失效一定会发生，也就是说，分区耐受性（P）是必须要保证的，而对于互联网应用来说，可用性也是需要保证的，分布式存储系统通常需要在一致性上做一些妥协和增强 Apache Cassandra 解决数据一致性的方案是，在用户写入数据的时候，将一个数据写入集群中的三个服务器节点，等待至少两个节点响应写入成功。用户读取数据的时候，从三个节点尝试读取数据，至少等到两个节点返回数据，并根据返回数据的时间戳，选取最新版本的数据。这样，即使服务器中的数据不一致，但是最终用户还是能得到一个一致的数据，这种方案也被称为最终一致性。 Micro service 单体架构缺点之一：一个巨型的应用必须把应用部署到大规模的服务器集群上。然后每个应用都需要与数据库建立连接，大量的应用服务器连接到数据库，会对数据库的连接产生巨大的压力，某些情况下甚至会耗尽数据库的连接 实施微服务最重要的是做好业务的模块化设计，如果业务关系没梳理好，模块设计不清晰，使用微服务架构很可能得不偿失，带来各种挫折 中台：-企业级能力复用平台","categories":[{"name":"后端架构","slug":"后端架构","permalink":"https://jiac3366.github.io/categories/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84/"}],"tags":[]},{"title":"K8s | 写入etcd的过程","slug":"Kubernetes/etcd写入过程","date":"2021-11-16T23:57:43.379Z","updated":"2021-12-19T16:08:43.422Z","comments":true,"path":"2021/11/17/Kubernetes/etcd写入过程/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/etcd%E5%86%99%E5%85%A5%E8%BF%87%E7%A8%8B/","excerpt":"","text":"写入etcd的过程 1、预检查 配额 限速 鉴权 包大小检查 2、 经过kvSever传到一致性模块（写请求发给了follower会被一致性模块转发给leader） 基于Raft paper 把请求放入raftLog(Memory暂存) 然后同时做2个操作 MsgProp发给其他follower 写一个WAL log(二进制的, 将y=9序列化),后台有异步操作落盘 3、其他follower接收到请求也做写自己raftLog 和WAL，最后给leader返回一个MsgAppResp 4、kvSever统计MatchIndex 是否超过半数follower确认,目前还是日志，没有写状态机 5、当超半数确认，raftLog中的状态从unstable –&gt;committed，写MVCC模块（状态机） 写treeIndex key-value形式 key是对象的key, value是版本信息，main revision（eg 最后一次版本号是4）和sub revision（第0次操作）, 所以可以get 某个key对应的revision Metadata的resourceVersion读的就是modified信息 generation历史变动：3版本创建 历史版本是3和4（有2版本） 写BoltDB key-value形式 key是revision value是这条记录的整个信息 为什么要写WAL呢，直接写MVCC不行吗？ 放个地方临时持久化，因为需等待其他follower半数以上同意，又要防止内存断电 当等到半数以上同意了，这时如果etcd挂了，大家已经确认的数据就可以直接从WAL中恢复 所以WAL是写入状态机之前等待协商的过程保证数据安全用的 6、落盘后，raftLog中的状态从committed–&gt;applied","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"K8s | 容器网络基础","slug":"Kubernetes/容器网络基础","date":"2021-11-16T23:51:18.931Z","updated":"2021-12-19T16:08:31.281Z","comments":true,"path":"2021/11/17/Kubernetes/容器网络基础/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/","excerpt":"","text":"32丨浅谈容器网络 每个容器（把每一个容器看做一台主机）都有“网络栈”，就包括了：网卡（Network Interface）、回环设备（LoopbackDevice）、路由表（Routing Table）和 iptables 规则。 如何把这些容器“连接”到 docker0 网桥上？——叫Veth Pair的虚拟设备 特点：被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，被用作连接不同 Network Namespace 的“网线” 其2端可以分别在容器和宿主机输入ifconfig查看，宿主机的网卡名字叫作 veth9c02e56，容器网卡名字叫eth0。并且，通过 brctl show 的输出，你可以看到这张网卡被“插”在了 docker0 上 容器A ping 容器B是通的，原理： 把docker0看成交换机，容器的网卡就是docker0交换机的端口，被限制在 NetworkNamespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换. 首先要查询容器A的路由表查询到容器B的IP的网段要转发的gateway（0.0.0.0），0.0.0.0意味着这是一条直连规则，凡是匹配到这条规则的 IP包，应该经过本机的 eth0 网卡，通过二层网络直接发往目的主机，前提是需要MAC地址 容器A通过 eth0 网卡发送ARP广播查找容器B的MAC，docker0负责转发到其他插在它上面的虚拟网卡eth0 网卡，是一个 Veth Pair，它的一端在容器的Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上，从而eth0可以认为是docker0网桥的一个端口 容器B因为网卡也插在docker0，回复ARP请求，容器A有了目的 MAC 地址，其 eth0 网卡就可以将数据包发出去 docker0处理转发，查询CAM 表（MAC地址表），转发到容器B的端口 可以打开iptables的TRACE 验证上述流程iptables - Wikipedia 心得：当你遇到容器连不通“外网”的时候，应该先试试 docker0 网桥能不能ping 通，然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则（容器里面也有路由表哦，用route命令查看）是不是有异常，往往就能够找到问题的答案了 容器的“跨主通信” 这个粉红色的Overlay Network有点像接在交换机上的路由器？ 课后：如果要在生产环境中使用容器的 HostNetwork 模式，需要做哪些额外的准备工作呢?","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"K8s | K8s跨主机通信原理","slug":"Kubernetes/K8s跨主机通信原理","date":"2021-11-16T23:49:02.369Z","updated":"2021-12-19T16:08:46.292Z","comments":true,"path":"2021/11/17/Kubernetes/K8s跨主机通信原理/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/K8s%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86/","excerpt":"","text":"“跨主通信”的原理 Flannel 支持三种后端实现（本节介绍后2种） Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现。 host-gw UDP（flanneld 扮演者路由器的角色） 这个 UDP 包的源地址，就是flanneld 所在的 Node 1 的地址，目的地址是 container-2 所在的宿主机 Node 2 的地址每台宿主机上的 flanneld，都监听着8285 端口 1、出现跨宿主机通信，包会被交给默认路由规则，进入 docker0 与宿主机的路由进行匹配，进入到一个叫作 flannel0的设备，接着交给Flannel进程（内核态向用户态的流动）在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备，只负责在操作系统内核和用户应用程序之间传递 IP 包。 2、flanneld 收到 IP 包的目的地址，就把它发送给了 Node 2宿主机， 它是如何知道这个 IP 地址对应的容器，是运行在 Node 2 上的？ 首先明白： Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”，子网与宿主机的对应关系，保存在 Etcddocker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网 flanneld 处理flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3 若 Node 1 和 Node 2 是互通的，从Node1就可以发送给Node2 缺点： 相比于两台宿主机之间的直接通信，多了一个额外的步骤，即 flanneld 的处理过程：由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝 VXLAN Virtual Extensible LAN（虚拟可扩展局域网）， Linux 内核本身就支持的一种网络虚似化技术，VXLAN 可以完全在内核态实现 UDP模式中在用户态进程flanneld:8285的封装和解封装的工作 在现有的三层网络之上，“覆盖”一层虚拟的、由内核VXLAN 模块负责维护的二层网络为在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）作用和跟前面的 flanneld 进程十分类似​差别在于封装和解封装的对象是二层数据帧；执行流程是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块） Node 2 启动并加入 Flannel 网络之后， Node 1（以及所有其他节点）的flanneld 就会添加一条到 Node2 VTEP设备IP的路由规则flannel.1 设备( VTEP 设备)替代了原flannel0 设备和主机eth0 VTEP 设备收到docker0发来的“原始 IP包”要想办法加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”，那么目的 VTEP 设备”的 MAC 地址是什么？因为Node1已有到Node2 VTEP的IP，所以自然要使用ARP，需要的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时自动添加在 Node 1上（不依赖 L3 MISS 事件和 ARP 学习）可以通过 ip 命令查看——ip neigh show dev flannel.1（ VTEP 设备和宿主机都有MAC地址哦） 加上“目的 VTEP 设备”的 MAC 地址，得到一个“内部数据帧”，但并不能在宿主机二层网络里传输 Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，让它载着完整的二层数据帧，通过宿主机的 eth0 网卡进行传输，把这次要封装出来的、宿主机对应的数据帧称为“外部数据帧”（Linux 内核会把这个数据帧封装进一个 UDP 包里发出去）Linux 内核会在“内部数据帧”前面，加上一个VXLAN 头表示“乘客”实际上是一个 VXLAN 要使用的数据帧，VXLAN 头里有个标志叫VNI，是 VTEP 设备识别某个数据帧是否归自己处理的重要标识。在 Flannel 中宿主机上的VTEP 设备都叫作 flannel.1 ，原因是这个“1”是VNI 标志的默认值 flannel.1 设备只知道另一端的 flannel.1 设备的 MAC 地址，却不知道对方的宿主机地址的IP地址， UDP 包该发给哪台宿主机呢？ flannel.1 设备实际扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自一个叫FDB（Forwarding Database）的转发数据库（相当于交换机的MAC表），bridge fdb 命令可以查看对方宿主机的IP FDB 信息，也是 flanneld 进程负责维护的bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:37(对方 VTEP 设备的MAC，由ip neigh show dev flannel.1查出来的) Linux封包发送（Node 2 的 MAC 地址，是 Node 1 的 ARP 表要学习的内容，无需 Flannel 维护） 对方接受 后两种模式其实都可以称作“隧道”机制，也是很多其他容器网络插件的基础。比如 Weave 的两种模式，以及 Docker 的 Overlay 模式。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"K8s | K8s权限控制模型 RBAC","slug":"Kubernetes/K8s权限控制模型 RBAC","date":"2021-11-16T23:46:32.111Z","updated":"2021-12-19T16:08:48.655Z","comments":true,"path":"2021/11/17/Kubernetes/K8s权限控制模型 RBAC/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/K8s%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B%20RBAC/","excerpt":"","text":"很多实际的容器化工作，都会要求你设计一个自己的编排对象，实现自己的控制器模式。 K8s 里新增和操作 API 对象，必须先了解RBAC 负责完成授权（Authorization）工作的机制，就是 RBAC ​ 3大概念（Role + RoleBinding + ServiceAccount） Role 产生作用的 Namepace 是：mynamespaceNamespace 并不会提供任何实际的隔离或者多租户能力 rules 字段，就是它所定义的权限规则在上面的例子里：允许“被作用者”，对 mynamespace 下面的 Pod 对象，进行 GET、WATCH 和 LIST 操作​ RoleBinding 可以指定上面Role的“被作用者” “User”字段，只是一个授权系统里的逻辑概念 大多数私有的使用环境中，我们只要使用 Kubernetes 提供的内置“用户”，就足够 可以通过外部认证服务，比如 Keystone来提供keystone? 也可以直接给 APIServer 指定一个用户名、密码文件 roleRef字段 绑定我们前面定义的 Role 对象（example-role） Role对权限的限制规则仅在自己的 Namespace 内有效，roleRef 也只能引用当前 Namespace 里的 Role 对象 Subject(User)——一般用ServiceAccount，是由 k8s负责管理的“内置用户” ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: namespace: mynamespace name: example-sa RoleBinding 下面的example-role在上面已经创建 实践：用Pod绑定按上述步骤创建的ServiceAccount Pod引用这个serviceAccount的name，创建成功后的Secret 对象（但cncamp课程演示 sa一旦被创建secrete就也一起被创建）（ServiceAccount 对应的用来跟 APIServer 进行交互的授权文件：Token）被mount到pod里面的/var/run/secret/k8s.io/sa( ，pod应用带着它访问apiserver，apiserver就知道你是谁Token 文件的内容一般是证书或者密码，以 Secret对象的方式保存在 Etcd kubectl describe pod sa-token-test -n mynamespace 可以查看该 ServiceAccount 的 tokeneg ：被 k8s自动挂载到了容器的/var/run/secrets/kubernetes.io/serviceaccount，容器里的应用就可以使用里面的 ca.crt 来访问 APIServer 了 tips: Pod 没有声明 serviceAccountName，Kubernetes 会自动在它的 Namespace 下创建一个名叫 default 的默认 ServiceAccount，然后分配给这个 Pod，有访问 APIServer 的绝大多数权限 你可以通过describe sa default查看对应的secret对象，也可以通过kubectl describe secret default-token-s8rbq查看对应的sa对象 用户组：一个 ServiceAccount，在 Kubernetes 里对应的“用户”的名字system:serviceaccount:&lt;ServiceAccount 名字 &gt; 而它对应的内置“用户组”的名字system:serviceaccounts:&lt;Namespace 名字 &gt;name: system:serviceaccounts就意味着这个 Role 的权限规则，作用于整个系统里的所有 ServiceAccount 更大的授权 ：对于非 Namespaced（Non-namespaced）对象（比如：Node），或者某一个 Role 想要作用于所有的 Namespace 的时候，我们又该如何去做授权呢？ ClusterRole和ClusterRoleBinding 用法跟 Role 和 RoleBinding 完全一样，只是没有了ns字段 k8s内置了很多为系统保留的 ClusterRole eg ：system:kube-scheduler绑定给 kube-system Namesapce下名叫 kube-scheduler 的 ServiceAccount（k8s调度器的 Pod 声明使用的 ServiceAccount） 1.cluster-admin；2. admin；3. edit；4. view务必要谨慎而小心地使用 cluster-admin（cluster-admin是最高权限（verbs=*）） Role 或者 ClusterRole 里，如果要赋予用户 (是User，不是ServiceAccount )example-user 所有权限，那就可以给它指定一个 verbs 字段的全集 verbs: [“get”, “list”, “watch”, “create”, “update”, “patch”, “delete”] 更细化的授权 这条规则的“被作用者”，只对名叫“my-config”的 ConfigMap 对象，有进行 GET 操作的权限","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"K8s | API对象的奥秘！","slug":"Kubernetes/API对象的奥秘！","date":"2021-11-16T23:45:06.990Z","updated":"2021-12-19T16:08:37.751Z","comments":true,"path":"2021/11/17/Kubernetes/API对象的奥秘！/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/API%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%A5%A5%E7%A7%98%EF%BC%81/","excerpt":"","text":"K8s是如何对 Resource、Group 和 Version 进行解析，从而找到 对象的定义呢 1、会匹配 API 对象的组核心 API 对象，比如：Pod、Node 等， Group 是“”（不需要Group）会直接在 /api 这个层级进行下一步的匹配过程 2、匹配到 API 对象的版本号同一种 API 对象可以有多个版本，这正是 Kubernetes 进行 API 版本化管理的重要手段。比如在 CronJob 的开发过程中，对于会影响到用户的变更就可以通过升级新版本来处理，从而保证了向后兼容 3、Kubernetes 会匹配 API 对象的资源类型 APIserver交互流程 1、授权、超时处理、审计 2、MUX 和 Routes 流程APIServer 完成 URL 和 Handler 绑定的场所 3、APIServer 进行Convert 工作，把用户提交的 YAML 文件，转换成一个叫作 Super Version 的对象，它正是该 API 资源类型所有版本的字段全集。这样用户提交的不同版本的 YAML 文件，就都可以用这个 Super Version 对象来进行处理 4、 Admission() 和 Validation() 操作 上节提到的的 Admission Controller 和 Initialize Validation，则验证这个对象里的各个字段是否合法只要一个 API 对象的定义能在 Registry 里查到，它就是一个有效的 Kubernetes API 对象 5、APIServer 会把验证过的 API 对象转换成用户最初提交的版本，进行序列化操作，并调用 Etcd 的 API 保存 自定义CRD 通俗说：为了让k8s认识这个对象 register.go pkg/apis/samplecrd/register.go 放置后面要用到的全局变量 doc.go 起到的是全局的代码生成控制的作用 被称为 Global Tagspkg/apis/samplecrd/doc.go,+k8s:deepcopy-gen=package 意思是，请为整个 v1 包里的所有类型定义自动生成 DeepCopy 方法；而+groupName=samplecrd.k8s.io，则定义了这个包对应的 API组的名字。 types.go pkg/apis/samplecrd/types.go ​由于 Global Tags，不需要再显式地加上 +k8s:deepcopy-gen=true 了 TypeMeta ObjectMeta Spec Cidr Gateway register.go——addKnownTypes() pkg/apis/samplecrd/v1/register.go 让客户端也能“知道CRD, APIServer 会自动帮我们完成在服务器端的注册 代码生成工具为资源类型生成clientset、informer 和 lister（详看PDF） 把其中的资源类型、GroupName 和 Version 替换成你自己的定义 总结自定义对象工作分2部分： 自定义资源类型的 API 描述组Group、版本（Version）、资源类型（Resource） 自定义资源类型的对象描述Spec、Status tip:用 kubebuild 自动生成项目框架，添加自己的 CRD 并实现 controller 即可","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"K8s | 声明式API与Kubernetes编程范式","slug":"Kubernetes/声明式API与Kubernetes编程范式","date":"2021-11-16T23:41:27.313Z","updated":"2021-12-19T16:08:35.617Z","comments":true,"path":"2021/11/17/Kubernetes/声明式API与Kubernetes编程范式/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/%E5%A3%B0%E6%98%8E%E5%BC%8FAPI%E4%B8%8EKubernetes%E7%BC%96%E7%A8%8B%E8%8C%83%E5%BC%8F/","excerpt":"","text":"先 kubectl create，再 replace 的操作，称为命令式配置文件操作。 什么才是“声明式 API”——kubectl apply 创建和修改都用kubectl apply kubectl replace 是使用新的 YAML 文件中的API 对象，替换原有的 API 对象，一次只能处理一个写请求 kubectl apply是执行一个对原有 API 对象的PATCH 操作，一次能处理多个写操作，并且具备 Merge 能力 Envoy——讲解一下声明式 API 在实际使用的重要意义 Envoy是一个高性能 C++ 网络代理，istio 的控制层（Control Plane）里的 Pilot 组件，能通过调用每个 Envoy容器的 API对Envoy 代理进行配置，实现微服务治理 Envoy以 sidecar 容器的方式运行在每被治理的应用Pod 中，通过配置 Pod 里的 iptables 规则，接管整个 Pod 的进出流量 DynamicAdmission Control ——“热插拔”式的 Admission 机制，也叫作：Initializer。 背景： K8s 的1个 Pod 或者1个 API 对象被提交给 APIServer ，有一些“初始化”性质的工作需要在它们被正式处理之前进行eg: 自动为所有 Pod 加上某些标签（Labels） Istio 项目要做的，就是在1个 Pod YAML 被提交给 K8s 之后，在里面自动加上 Envoy 容器的配置 Istio 会将 Envoy 容器本身的定义，以 ConfigMap 的方式保存，这个 ConfigMap 的 data 部分，正是一个 Pod 对象的一部分定义，所以会有重叠，所以更新用户的 Pod 对象的时候，必须使用 PATCH API 来完成 Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 K8s 中，其中这个Initializer的工作过程如下： 这个 envoy-initializer 使用的 envoy-initializer:0.0.1 镜像，是一个事先写好的“自定义控制器”（Custom Controller），在下一篇文章中讲解 initializer 控制器 1、不断检查有无新pod创建, 实际就是死循环，逻辑伪代码 2、从 APIServer 中拿ConfigMap的data 3、将 ConfigMap 里存储的 containers 和 volumes 字段，直接添加进一个空的Pod 对象里 4、使用k8sAPI，使用新旧两个 Pod 对象，生成一个 TwoWayMergePatch 5、调用 K8s 的 Client，发起一个 PATCH 请求 ps、指定要对什么样的资源起作用 成功创建后，新创建的 Pod 的 Metadata 上就有这个Initializer的name，但initializer 里完成了要做的操作后，一定要记得将metadata.initializers.pending 标志清除掉 ps、在具体的 Pod 的 Annotation 里声明要使用哪个 Initializer, eg：Annotation写initializer.kubernetes.io/envoy=true, 就会使用到我们前面所定义的envoy-initializer 如何理解“Kubernetes 编程范式”，如何为 Kubernetes 添加自定义 API 对象，编写自定义控制器，正是这个晋级过程中的关键点","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"K8s | StatefulSet是什么？","slug":"Kubernetes/StatefulSet是什么？","date":"2021-11-16T23:40:12.242Z","updated":"2021-12-19T16:08:54.943Z","comments":true,"path":"2021/11/17/Kubernetes/StatefulSet是什么？/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/StatefulSet%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","excerpt":"","text":"18 深入理解StatefulSet（一）：拓扑状态 实例之间有不对等关系，以及实例对外部数据有依赖关系的应用，就被称为“有状态应用” Stateful Application StatefulSet把真实世界的应用抽象成2种情况 在部署“有状态应用”的时候，应用的每个实例拥有唯一并且稳定的“网络标识”，是一个非常重要的假设 拓扑状态 存储状态（下一节讲） Service被访问的方式有2种 一、Service的虚拟 IP 二、Service的DNS 访问这个dns，正是这个 Service 的 VIP，和（一）一致 访问这个dns直接解析出被代理 Pod 的 IP 地址，这就叫 Headless Service 小实践 先创建 Headless Service Headless Service 不需要分配一个 VIP，而是可以直接以 DNS 记录的方式解析出被代理 Pod 的 IP 地址​仍是一个标准 Service 的 YAML 文件。只不过它的 clusterIP 字段的值是：None 创建后，它所代理的所有 Pod（label selector选出来的） 的 IP 地址，都会被绑定一个DNS 记录 再创建statefulset 只要知道了一个 Pod 的名字，以及它对应的 Service 的名字，就可通过这条 DNS 记录访问到 Pod 的 IP 和deployment YAML的唯一区别，就是多了一个 serviceName=nginx 字段，StatefulSet控制器会使用这个叫nginx 的Headless Service 来保证 Pod 的可解析身份（DNS记录） 这时候创建一个pod 对statefulset维护的2个pod nslookup 和删除这2个pod后再nslookup 结果一致，说明Kubernetes 就成功地将 Pod 的拓扑状态（比如：哪个节点先启动，哪个节点后启动），按照 Pod 的“名字 + 编号”的方式固定了下来。注意：解析到的Pod 的 IP 地址，并不是固定的，所以对于“有状态应用”实例的访问，你必须使用 DNS 记录或者 hostname 的方式","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"K8s | Depolymnet、ReplicaSet、Pod之间的关系","slug":"Kubernetes/Depolymnet、ReplicaSet、Pod之间的关系","date":"2021-11-16T23:38:28.668Z","updated":"2021-12-19T16:08:40.799Z","comments":true,"path":"2021/11/17/Kubernetes/Depolymnet、ReplicaSet、Pod之间的关系/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/Depolymnet%E3%80%81ReplicaSet%E3%80%81Pod%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/","excerpt":"","text":"只有容器能保证自己始终是 Running 状态的前提下，ReplicaSet 调整 Pod 的个数才有意义也正是 Deployment 只允许容器的 restartPolicy=Always的原因 deployment状态字段 kubectl scale deployment nginx-deployment –replicas=4 水平扩展 kubectl rollout status deployment/nginx-deployment查看 nginx-deployment 的状态变化 ReplicaSet 的 DESIRED、CURRENT 和 READY 字段的含义，和 Deployment 一致。相比之下，Deployment 只是在 ReplicaSet 的基础上，添加了 UP-TO-DATE 这个跟版本有关的状态字段 使用 Pod 的 Health Check 机制检查应用的运行状态，当“滚动更新”停止，旧版本还能继续服务容器 Running 状态时，但服务很有可能尚未启动，“滚动更新”的效果也就达不到了 Deployment、ReplicaSet 和 Pod 的关系. Deployment 的控制器控制的是 ReplicaSet 的数目（描述应用的版本），以及每个ReplicaSet 的属性（来保证 Pod 的副本数量）：Deployment 控制 ReplicaSet（版本），ReplicaSet 控制 Pod（副本数）——deploy的yaml文件中同时定义replicaset和container，replicaset的replicas字段就是控制副本数的 1个应用的版本，对应的是1个 ReplicaSet，这个版本应用的 Pod 数量，则由ReplicaSet 通过它自己的控制器（ReplicaSet Controller）来保证 对 Deployment 进行的每一次更新操作，都会生成一个新的 ReplicaSet 对象,会比较浪费资源 1、kubectl rollout在更新 Deployment 前，先执行 kubectl rollout pause deployment/nginx-deployment 让Deployment 处于“暂停”状态，这时对 Deployment 的所有修改，都不会触发新的“滚动更新”，也不会创建新的 ReplicaSet最后执行 kubectl rollout resume deploy/nginx-deployment 恢复回来 2、Deployment 对象字段， spec.revisionHistoryLimitk8s为 Deployment 保留的“历史版本”个数，设置为 0，就再也不能做回滚操作了","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"K8s | 控制器模型是怎样的？","slug":"Kubernetes/控制器模型是怎样的？","date":"2021-11-16T23:36:56.605Z","updated":"2021-12-19T16:08:25.190Z","comments":true,"path":"2021/11/17/Kubernetes/控制器模型是怎样的？/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/%E6%8E%A7%E5%88%B6%E5%99%A8%E6%A8%A1%E5%9E%8B%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F/","excerpt":"","text":"CRD 就是一个专门用来定义 Schema 的一个特殊的 API 对象 控制循环——“ReconcileLoop”（调谐循环）或者“Sync Loop”（同步循环） 被控制对象的定义，则来自于一个“模板”。 控制循环最后的执行结果，要么创建、更新一些Pod（或者其他的 API 对象、资源），要么删除一些已经存在的 Pod（或者其他的API 对象、资源）。 Deployment 这样的一个控制器，上半部分是控制器定义（包括期望状态），下半部分的是被控制对象的模板 eg: Deployment 里的 template 字段（PodTemplate（Pod 模板）），被这个 Deployment 管理的 Pod 实例，都是根据这个template 字段的内容创建的 问题： Kubernetes 使用的这个“控制器模式”，跟我们平常所说的“事件驱动”，有什么区别和联系吗？ 相当于select和epoll的区别 事件往往是一次性的，如果操作失败比较难处理，但是控制器是循环一直在尝试的，更符合kubernetes声明式API，最终达到与声明一致 控制器主动获取pod状态，在这个集群中，有那么多pod，某个pod在某一时刻状态有变，怎样及时通知到控制器呢？informer机制","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"K8s | 深入理解Pod对象","slug":"Kubernetes/深入理解Pod对象","date":"2021-11-16T23:33:26.186Z","updated":"2021-12-19T16:08:33.497Z","comments":true,"path":"2021/11/17/Kubernetes/深入理解Pod对象/","link":"","permalink":"https://jiac3366.github.io/2021/11/17/Kubernetes/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Pod%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"13 为什么需要Pod？ 背景：容器的“单进程模型”容器没有管理多个进程的能力，因为容器里 PID=1 的进程就是应用本身，其他的进程都是这个 PID=1 进程的子进程，用户进程没有管理其他进程的能力 解决：k8s把协同工作在同1个“进程组”的多个线程等价为1个Pod的多个容器，解决典型的成组调度（gang scheduling）问题，在Pod的层面进行资源分配（多个容器加起来需要的总资源作为一个原子调度单位）具有“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace（比如，一个容器要加入另一个容器的 Network Namespace） 重要：Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume| A加入B的前提是B首先启动，所以为了保持Pod 里的多个容器就是对等关系： Pod 的实现需要使用一个中间容器，这个容器叫作 Infra 容器。在这个 Pod 中，Infra 容器永远都是第一个被创建的容器，用户定义的容器通过 Join Network Namespace 的方式，与 Infra 容器关联在一起，要为 Kubernetes 开发一个网络插件时只需要关注如何配置 Infra 容器的 Network Namespace 即可. 用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器 例子1：war包和tomcat分别做成2个镜像，war包是个 Init Container 类型的容器，它会按顺序逐一启动，等到他们启动并退后，spec.containers 定义的用户容器才启动，后面的tomcat（与war容器挂载了同一Volume）就看到Volume对应的宿主机目录中有war包 上述例子是容器设计模式里最常用的一种模式：sidecarsidecar 指的就是我们可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作。很多与 Pod 网络相关的配置和管理，也都可以交给 sidecar 完成，而完全无须干涉用户容器。最典型的例子莫过于 Istio 这个微服务治理项目 例子2：一个最基本的日志收集工作：A往Volume中写日志，B也声明挂载同一个 Volume，然后B可以读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。 虚拟机和容器区别： 部署方式：容器无法像虚拟机那样完全模拟本地物理机环境 一个运行在虚拟机里的应用是被管理在 systemd 或者supervisord 之下的一组进程，而不是一个进程。Swarm 这种单容器的工作方式，就难以描述真实世界这种复杂的应用架构，需要把一个运行在虚拟机里的应用迁移到 Docker 容器中时，要仔细分析到底有哪些进程（组件）运行在这个虚拟机 Pod实际上是在扮演“虚拟机”的角色；而容器只是一个进程，扮演这个虚拟机里运行的用户程序。 14 深入理解Pod对象（一）（Pod对象字段） API对象到底哪些属性属于 Pod 对象，而又有哪些属性属于Container 呢？ ——把 Pod 看成传统环境里的“机器”、把容器看作是运行在这个“机器”里的“用户程序” 常用的Pod字段：(在第一个spec下) 凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的 NodeSelector: a:b 这个 Pod 永远只能运行在携带了“a: b”标签（Label）的节点 HostAliases: 定义了 Pod 的 hosts 文件（比如 /etc/hosts），设置 hosts 文件一定要通过这种方法 shareProcessNamespace: true Pod 里的容器共享 PID Namespace整个 Pod 里的每个容器的进程，对于所有容器来说都是可见的​k attach -it nginx -c shell？？为啥无法交互了 Pod共享宿主机的 Network、IPC 和 PID Namespace hostNetwork: true hostIPC: true hostPID: true container字段 –在deployment对象的第二个spec下，以下字段是container字段的属性 Image（镜像） Command（启动命令） workingDir（容器的工作目录） Ports（容器要开的端口） volumeMounts（容器要挂载的 Volume） imagePullPolicy 默认值是 Always，即每次创建 Pod 都重新拉取一次镜像 Never ：Pod 永远不会主动拉取这个镜像 IfNotPresent：只有宿主机上不存在这个镜像时才拉取 Lifecycle postStart指在容器启动后，立刻执行一个指定的操作并不严格保证顺序，在 postStart 启动时，ENTRYPOINT 有可能还没有结束 preStop发生的时机是容器被杀死之前（比如，收到了 SIGKILL 信号）会阻塞当前的容器杀死流程，直到这个 Hook 定义操作完成之后，才允许容器被杀死，和postStart不一样 Status pod.status.phase，是 Pod 的当前状态 Pending 已提交给etcd,因为某种原因而不能被顺利创建, 比如调度不成功。 Running Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中 Succeeded Pod 里的所有容器都正常运行完毕，并且已经退出了（1次性任务） Failed Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。需要查看 Pod 的 Events和日志 Unknown Pod 的状态不能持续地被 kubelet 汇报给kube-apiserver——主从节点（Master 和 Kubelet）间的通信出现了问题 Conditions type：PodScheduled、Ready、Initialized，以及 Unschedulable,主要用于描述造成当前 Status 的具体原因是什么。 Pending: 对应的 Condition 是 Unschedulable， 调度出现了问题。 Ready： Pod 不仅已经正常启动（Running 状态），而且已经可以对外提供服务了 15 深入理解Pod对象（二） Projected Volume ——具备自动更新的能力 这些特殊 Volume 的作用，是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被Kubernetes“投射”（Project）进入容器中的 Secret 数据库的验证信息 创建方式 在Pod yaml：先在volumes字段声明挂载的 是projected 类型，并为 Secret 对象指定的名字，再kubectl create secret generic user –from-file=./username.txt，其中user 是为 Secret 对象指定的名字 单独YAML 通过挂载方式进入到容器里的 Secret， kubelet 组件在定时维护这些 Volume一旦其对应的 Etcd 里的数据被更新，这些 Volume 里的文件内容，同样也会被更新，但会有一定的延时。所以在编写应用程序时，在发起数据库连接的代码处写好重试和超时的逻辑，绝对是个好习惯 ConfigMap 环境变量 与Secret基本一致，区别是不需要加密其中的信息 Downward API 声明要暴露 Pod 的某些信息（Pod 里的容器进程启动之前就能确定的信息）给容器，具体的可以查阅一下官方文档.例如 使用 fieldRef 声明： spec.nodeName - 宿主机名字 status.hostIP - 宿主机 IP metadata.name - Pod 的名字 metadata.namespace - Pod 的 Namespace等 使用 resourceFieldRef 声明： 容器的 CPU limit 容器的 CPU request 容器的 memory limit 容器的 memory request ServiceAccountToken（特殊的Secret对象）？？？有点模棱两可 Service Account（k8s进行权限分配的对象） 的授权信息（Token）和文件，实际保存在ServiceAccountToken 从容器里直接访问Kubernetes 的 API 需要ServiceAccountToken的TokenKubernetes 其实在每个 Pod 创建的时候，自动在它的spec.volumes 部分添加上了默认 ServiceAccountToken 的定义，然后自动给每个容器加上了对应的 volumeMounts 字段。简单说，默认就加载了授权信息可以访问k8sAPI 容器健康检查和恢复机制 为 Pod 里的容器定义一个健康检查“探针”（Probe），kubelet 根据 Probe 的返回值决定容器的状态 健康检查 livenessProbe字段，kubelet 根据 健康检查“探针”Probe 的返回值决定容器的状态 readinessProbe，决定的这个 Pod 是不是能被通过 Service 的方式访问到，而并不影响 Pod 的生命周期 Pod 恢复机制 Pod 的恢复过程是重新创建容器，永远都是发生在当前节点上，除非使用Deployment控制器 restartPolicy pod.spec.restartPolicy默认是Always。但如果只计算 1+1=2，计算完成输出结果后退出，变成 Succeeded 状态；所以设为Always没有意义 OnFailure: 只在容器 异常时才自动重启容器 Never: 从来不重启容器需要关心容器退出后的日志、文件和目录时这样设置 列表信息的STATUE字段 Failed的场景 运行1个容器的pod在restartPolicy=Never时出现 运行多个容器的pod中所有容器异常 （运维人员预先定义）PodPreset + （开发人员编写,标记上相关的selector）pod = 完整的pod yamlPodPreset 里定义的内容会在 Pod API 对象被创建之前追加在这个对象本身上，不会影响任何 Pod 的控制器的定义，比如这个Pod的 Deployment 对象本身是永远不会被 PodPreset改变，k8s还会Merge多个 PodPreset 要做的修改，如果它们要做的修改有冲突，这些冲突字段就不会被修改","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"","slug":"Golang/1108. IP 地址无效化","date":"2021-10-26T13:20:32.639Z","updated":"2021-10-26T13:44:32.852Z","comments":true,"path":"2021/10/26/Golang/1108. IP 地址无效化/","link":"","permalink":"https://jiac3366.github.io/2021/10/26/Golang/1108.%20IP%20%E5%9C%B0%E5%9D%80%E6%97%A0%E6%95%88%E5%8C%96/","excerpt":"","text":"1108. IP 地址无效化知识点 golang string和[]byte的区别？ 如何转换？ 根据[golang string和]byte的对比 - 张伯雨 - 博客园 (cnblogs.com)可知： string类型无法修改其中的某个字符，当我们操作的粒度小到具体1个字符时，用[]byte 代码实现 申请一块[]byte的内存，j作为遍历指针 直接遍历IP地址 if 遇到”.”，将”.”更换为 “[“ + “.” + “]” TIPS：我们注意，IPv4地址一般三个“.”，这里需要在原长度的基础上加6. 12345678910111213141516n,j := len(address), 0 rets := make([]byte, n+6) for i:=0; i&lt;n;i++ &#123; if address[i] == &#x27;.&#x27; &#123; rets[j]=&#x27;[&#x27; j++ rets[j]=&#x27;.&#x27; j++ rets[j]=&#x27;]&#x27; j++ &#125; else &#123; rets[j] = address[i] j++ &#125; &#125; return string(rets) ​","categories":[],"tags":[]},{"title":"","slug":"Golang/347. 前 K 个高频元素","date":"2021-10-25T12:36:56.452Z","updated":"2021-10-26T13:35:42.385Z","comments":true,"path":"2021/10/25/Golang/347. 前 K 个高频元素/","link":"","permalink":"https://jiac3366.github.io/2021/10/25/Golang/347.%20%E5%89%8D%20K%20%E4%B8%AA%E9%AB%98%E9%A2%91%E5%85%83%E7%B4%A0/","excerpt":"","text":"347. 前 K 个高频元素思路 此题考查堆相关算法的TopK问题 1、实现最小堆 2、遍历所给的数据 若堆不满，直接入堆 若堆满，if 当前当前元素 &gt; 最小堆的root：入堆 3、输出堆中的元素即为TopK的元素 实现堆这里简单介绍golang如何实现最小堆，如已掌握可以跳过。题解在最下方。 官方**&quot;container/heap&quot;**包提供的堆方法（以下Interface1）： 12345type Interface1 interface &#123; sort.Interface Push(x interface&#123;&#125;) // add x as element Len() Pop() interface&#123;&#125; // remove and return element Len() - 1.&#125; 而继续点开sort.Interface： 123456789type Interface interface &#123; // Len is the number of elements in the collection. Len() int // Less reports whether the element with // index i should sort before the element with index j. Less(i, j int) bool // Swap swaps the elements with indexes i and j. Swap(i, j int)&#125; 可以得知，我们需要实现不少方法 总体思路是 自定义一个数组，数组中需要实现上述2个接口的方法，接口分别是Interface1和Interface2，取名为PriorityQueue 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// An Item is something we manage in a priority queue.type Item struct &#123; value string // The value of the item; arbitrary. priority int // The priority of the item in the queue. // The index is needed by update and is maintained by the heap.Interface methods. index int // The index of the item in the heap.&#125;// A PriorityQueue implements heap.Interface and holds Items.type PriorityQueue []*Itemfunc (pq PriorityQueue) Len() int &#123; return len(pq) &#125;func (pq PriorityQueue) Less(i, j int) bool &#123; // We want Pop to give us the highest, not lowest, priority so we use greater than here. return pq[i].priority &gt; pq[j].priority&#125;func (pq PriorityQueue) Swap(i, j int) &#123; pq[i], pq[j] = pq[j], pq[i] pq[i].index = i pq[j].index = j&#125;func (pq *PriorityQueue) Push(x interface&#123;&#125;) &#123; n := len(*pq) item := x.(*Item) item.index = n *pq = append(*pq, item)&#125;func (pq *PriorityQueue) Pop() interface&#123;&#125; &#123; old := *pq n := len(old) item := old[n-1] item.index = -1 // for safety *pq = old[0 : n-1] return item&#125;// update modifies the priority and value of an Item in the queue.func (pq *PriorityQueue) update(item *Item, value string, priority int) &#123; item.value = value item.priority = priority heap.Fix(pq, item.index)&#125; 使用container/heap堆化、操作PriorityQueue 1234567891011121314151617181920212223242526272829303132333435363738// This example creates a PriorityQueue with some items, adds and manipulates an item,// and then removes the items in priority order.func Example_priorityQueue() &#123; // Some items and their priorities. items := map[string]int&#123; &quot;banana&quot;: 3, &quot;apple&quot;: 2, &quot;pear&quot;: 4, &#125; // Create a priority queue, put the items in it, and // establish the priority queue (heap) invariants. pq := make(PriorityQueue, len(items)) i := 0 for value, priority := range items &#123; pq[i] = &amp;Item&#123; value: value, priority: priority, index: i, &#125; i++ &#125; heap.Init(&amp;pq) // Insert a new item and then modify its priority. item := &amp;Item&#123; value: &quot;orange&quot;, priority: 1, &#125; heap.Push(&amp;pq, item) pq.update(item, item.value, 5) // Take the items out; they arrive in decreasing priority order. for pq.Len() &gt; 0 &#123; item := heap.Pop(&amp;pq).(*Item) fmt.Printf(&quot;%.2d:%s &quot;, item.priority, item.value) &#125; // Output: // 05:orange 04:pear 03:banana 02:apple&#125; 回到题目12345678910111213141516171819202122232425262728293031323334353637383940414243import ( &quot;container/heap&quot; &quot;fmt&quot; &quot;sort&quot;)// An IntHeap is a min-heap of ints.type IntHeap []intfunc (h IntHeap) Len() int &#123; return len(h) &#125;func (h IntHeap) Less(i, j int) bool &#123; return h[i] &lt; h[j] &#125;func (h IntHeap) Swap(i, j int) &#123; h[i], h[j] = h[j], h[i] &#125;func (h *IntHeap) Push(x interface&#123;&#125;) &#123; // Push and Pop use pointer receivers because they modify the slice&#x27;s length, // not just its contents. *h = append(*h, x.(int))&#125;func (h *IntHeap) Pop() interface&#123;&#125; &#123; old := *h n := len(old) x := old[n-1] *h = old[0 : n-1] return x&#125;// This example inserts several ints into an IntHeap, checks the minimum,// and removes them in order of priority.func main() &#123; h := &amp;IntHeap&#123;100,16,4,8,70,2,36,22,5,12&#125; fmt.Println(&quot;\\nHeap:&quot;) heap.Init(h) fmt.Printf(&quot;最小值: %d\\n&quot;, (*h)[0]) //for(Pop)依次输出最小值,则相当于执行了HeapSort fmt.Println(&quot;\\nHeap sort:&quot;) for h.Len() &gt; 0 &#123; fmt.Printf(&quot;%d &quot;, heap.Pop(h)) &#125;&#125;","categories":[],"tags":[]},{"title":"容器技术 | Docker容器知识个人笔记","slug":"容器技术/Docker知识汇总","date":"2021-10-14T15:51:55.571Z","updated":"2021-12-19T16:07:27.377Z","comments":true,"path":"2021/10/14/容器技术/Docker知识汇总/","link":"","permalink":"https://jiac3366.github.io/2021/10/14/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/Docker%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB/","excerpt":"","text":"容器 = rootfs(静态视图) + Ns和Cgroups(动态视图) Namespace docker创建容器进程时，实际是指定了这个进程所需要启用的一组 Namespace 参数，是一种特殊的进程 int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 隔离的Namespace中的一些命令比如ping，netstat不受docker控制，容器是单进程意思是只有1个进程是可控的 缺点：共享宿主机内核，win容器应该不能在linux跑 Cgroup CPU/Memory watch ‘ps -aux|grep malloc|grep -v grep’ 查看正在分配内存的应用，这个应用的二进制文件名叫malloc 缺点： 提及最多的自然是 /proc 文件系统，/proc 文件系统不了解 Cgroups 限制的存在容器里执行 top 指令，显示的信息居然是宿主机的 CPU 和内存数据 课后问题：如何修复容器中的 top 指令以及 /proc 文件系统中的信息 A：top 是从 /prof/stats 目录下获取数据，所以道理上来讲，容器不挂载宿主机的该目录就可以了。lxcfs就是来实现这个功能的，做法是把宿主机的 /var/lib/lxcfs/proc/memoinfo 文件挂载到Docker容器的/proc/meminfo位置后。容器中进程读取相应文件内容时，LXCFS的FUSE实现会从容器对应的Cgroup中读取正确的内存限制.（改变容器top读取数据的位置） Mount ns Mount ns跟其他 ns略有不同：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。但我们希望的是：每当创建一个新容器时看到的文件系统就是一个独立的隔离环境，而不是默认继承自宿主机的文件系统。 Mount ns 对 chroot 的不断改良，做到默认挂载一个宿主机目录到容器根目录 chroot $HOME/test /bin/bash使用 $HOME/test 目录作为 /bin/bash进程(容器进程)的根目录 目前，为了容器根目录更真实，一般挂载一个完整操作系统的文件系统（比如 Ubuntu16.04 的 ISO）——“容器镜像”，更专业就叫rootfs（根文件系统）。rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核，所以容器镜像不含内核，同一台机器上的所有容器，都共享宿主机操作系统的内核 Docker核心原理就是为待创建的用户进程： 启用 Linux Namespace 设置指定的 Cgroups 参数 切换进程的根目录（Change Root）–优先用系统调用pivot_root ，没有就用chroot Union FS 联合文件系统 AuFS: 镜像的层（5个）都放置在 /var/lib/docker/aufs/diff 目录下，然后被联合挂载（1个）在 /var/lib/docker/aufs/mnt 里面 ？？？？例子中的可读写层 ID 6e3be5d2ecccae7怎么来的 5合1是如何做到的？信息记录在 /sys/fs/aufs 通过cat /proc/mounts| grep aufs找到/var/lib/docker/aufs/mnt/id 这个id（例子中的6e3be5d2ecccae7）的挂载信息，得到si=972c6d361e6b32ba 再通过cat /sys/fs/aufs/si_972c6d361e6b32ba/br[0-9]*得到 这就是宿主机存放这个镜像的层的真正文件 OverlayFS docker创新点：设计了增量rootfs，用到了Union FS rootfs由三部分组成 读写层（容器层），是专门用来存放你修改 rootfs 后产生的增量，无论是增、删、改，会被提交到hub被其他人使用 Init层（在2大层中间），专门用来存放 /etc/hosts、/etc/resolv.conf 等，不会提交，仅对当前容器有效 只读层（镜像层） 删除只读层：要删除只读层里一个名叫 foo 的文件，实际上是在可读写层创建了一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文件“遮挡”起来，“消失”了，即“ro+whiteout”的方式 修改只读层：找到，就复制到容器层中，修改，修改的结果就会作用到下层的文件。即“copy on write”的方式 Dockerfile 问题：SHELL 和 VOLUME命令 CMD和ENTRYPOINTT Docker 会为你提供一个隐含的 ENTRYPOINT，即：/bin/sh -c.不指定 ENTRYPOINT 时，CMD 的内容就是 ENTRYPOINT 的参数，实际上运行在容器里的完整进程是：/bin/sh -c CMD 每个原语执行后，都会生成一个对应的镜像层。即使原语本身并没有明显地修改文件的操作（比如，ENV 原语），它对应的层也会存在。只不过在外界看来，这个层是空的 docker commit 提交增量更新 这里你对镜像roofs做的修改就是copt-on-write，init层避免了 Docker 对 /etc/hosts 等文件做的修改也一起提交. 它发生在宿主机空间，由于 Mount ns的隔离作用，宿主机不知道有目录绑定到容器中，也就是说，宿主机认为容器中可读写层的 /test 目录（/var/lib/docker/aufs/mnt/[可读写层 ID]/test），始终是空的 docker exec原理： 加入到一个某个进程已有的 Namespace 当中，达到“进入”这个进程所在容器的目的 docker inspect –format ‘‘ [容器id] ——查看容器进程iddocker inspect [容器id] | grep -I pid ls -l /proc/容器进程id/ns ——查看这个容器真实的 Namespace 文件, 这样就可以依靠系统调用做有意义的事了 系统调用：setns()，可以指定一个进程进入另一个进程的ns docker提供了：-net参数 让你启动一个容器并“加入”到另一个容器的net ns； –net=host，不会启动net ns，就意味会和宿主机直接共享网络栈docker run -it –net container:4ddf4638572d busybox ifconfig docker volume原理： 就算开启了 Mount ns，在执行 chroot（或者 pivot_root）之前，容器进程一直可以看到宿主机上的整个文件系统 指定方式 docker run -v /test … ——默认在宿主机上创建一个临时目录 /var/lib/docker/volumes/[VOLUME_ID]/_data，然后把它挂载到容器的 /test 目录上 docker run -v /home:/test … 相关执行顺序：容器启动 –&gt; 5合1准备好容器的roofs –&gt; /home挂载到/var/lib/docker/aufs/mnt/[可读写层 ID]/test （此时mount ns已开启，挂载事件只在这个容器里可见，在宿主机上看不见容器内部的这个挂载点，保证了容器的隔离性不会被 Volume 打破）在复习一下？？？inode知识盲区？？？–&gt;chroot系统调用 找的顺序：启动–&gt;docker volume ls 第一个id –&gt;ls /var/lib/docker/volumes/id/_data/ 08inode知识盲区 容器声明的 Volume 的挂载点虽然出现在读写层，但容器 Volume 里的信息，并不会被 docker commit 提交掉；但这个挂载点目录 /test 本身，则会出现在新的镜像当中","categories":[{"name":"容器技术","slug":"容器技术","permalink":"https://jiac3366.github.io/categories/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"}],"tags":[]},{"title":"CPU上下文切换（下）","slug":"Linux性能优化/3 CPU上下文切换（下）","date":"2021-10-13T12:07:57.248Z","updated":"2021-12-19T16:29:27.991Z","comments":true,"path":"2021/10/13/Linux性能优化/3 CPU上下文切换（下）/","link":"","permalink":"https://jiac3366.github.io/2021/10/13/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/3%20CPU%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2%EF%BC%88%E4%B8%8B%EF%BC%89/","excerpt":"","text":"vmstat用来分析系统的内存使用情况,也常用来分析CPU 上下文切换和中断的次数 cs（context switch）是每秒上下文切换的次数。 in（interrupt）则是每秒中断的次数。 r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。 若大于CPU数说明存在CPU竞争 b（Blocked）则是处于不可中断睡眠状态的进程数 us(user) sy 系统CPU使用率 进一步每个进程的详细情况 使用pidstatpidtstat -w 5 隔5s输出一组数据 单位: 次/秒-w 参数表示输出进程切换指标，而 -u 参数则表示输出 CPU 使用指标默认显示进程指标数据，加-t才输出线程指标 ——pidstat -wt 1 cswch，自愿上下文切换，eg: 资源不足 nvcswch非自愿上下文切换，eg:时间片耗尽 sysbench 是一个多线程的基准测试工具，一般用来评估不同系统参数下的数据库负载情况 sysbench –threads=10 –max-time=300 threads run pidstat 只是一个进程的性能分析工具，而中断发生在内核态，怎样才能知道中断发生的类型呢？","categories":[{"name":"Linux性能优化","slug":"Linux性能优化","permalink":"https://jiac3366.github.io/categories/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"}],"tags":[]},{"title":"CPU上下文切换（上）","slug":"Linux性能优化/2 CPU上下文切换（上）","date":"2021-10-12T12:10:12.922Z","updated":"2021-10-14T15:59:05.382Z","comments":true,"path":"2021/10/12/Linux性能优化/2 CPU上下文切换（上）/","link":"","permalink":"https://jiac3366.github.io/2021/10/12/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/2%20CPU%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"操作系统管理的“任务”有哪些？ 进程和线程 硬件触发信号 系统负载升高因素之一：上下文切换频繁，缩短进程真正运行的时间 开销排名：进程上线文切换&gt;同进程线程上线文切换&gt;中断上线文切换&gt;内核模式切换&gt;协程上线文切换&gt;用户态函数调用上下文切换 根据“任务”不同，上下文切换也就有不同的场景 进程上下文切换 进程既可以在用户空间运行，又可以在内核空间中运行。 和系统调用（特权模式切换）的区别： 1、进程上下文切换，是指从一个进程切换到另一个进程运行，而系统调用过程中一直是同一个进程在运行。 2、切换的资源 系统调用的过程发生了 2次CPU 上下文切换。每次切换CPU寄存器和内核状态（内核资源）。第一次：CPU 寄存器先保存原来用户态的指令位置，为了执行内核态代码， 需要更新为内核态指令的新位置。最后跳转到内核态运行内核任务。第二次同理。 进程上下文切换不仅切换CPU寄存器和内核状态，还需要切换虚拟内存、用户栈、全局变量（用户资源），当虚拟内存刷新后，TLB（负责从虚拟地址转换到物理地址）也要刷新。 什么时候会切换进程上下文？调度算法学一波！！ 进程时间片耗尽 进程需要的内存（系统资源）不满足–&gt;被挂起 主动sleep –&gt;被挂起 更高优先级的来了–&gt;被挂起 硬件中断发生–&gt;被挂起 线程上下文切换 线程上下文分2种 前后的线程不属于同一个进程–&gt;等同进程切换 前后的线程属于同一个进程–&gt;只切换线程的栈、寄存器等不共享的资源 中断上下文切换 中断上下文只包括内核态中断处理程序执行所必需的状态（只发生在内核态），包括 CPU 寄存器、 内核堆栈、硬件中断参数等，并不涉及到进程的用户态，所以即便硬件中断打断了进程的执行，也不用保存和恢复这个进程的虚拟内存、全局变量等用户态资源.","categories":[{"name":"Linux性能优化","slug":"Linux性能优化","permalink":"https://jiac3366.github.io/categories/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"}],"tags":[]},{"title":"什么是平均负载？","slug":"Linux性能优化/1 平均负载","date":"2021-10-11T12:00:53.305Z","updated":"2021-12-19T16:38:13.092Z","comments":true,"path":"2021/10/11/Linux性能优化/1 平均负载/","link":"","permalink":"https://jiac3366.github.io/2021/10/11/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/1%20%E5%B9%B3%E5%9D%87%E8%B4%9F%E8%BD%BD/","excerpt":"","text":"平均负载 uptime:平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数， 也就是平均活跃进程数. 可运行状态（Running/Runnable）的进程：正在使用 CPU 或者正在等待 CPU 的进程 不可中断状态（Disk Sleep）的进程：eg:正在写数据的进程，等待硬件设备的 I/O 响应系统对进程和硬件设备的一种保护机制 平均活跃进程数——单位时间内的活跃进程数 ，三个数字分别代表1 5 15分钟的平均负载 当为2时，对于2核CPU，刚好全被占用，对于4核数，空闲50%，对于1核，一半进程分配不到CPU 查看核数：grep ‘model name’ /proc/cpuinfo | wc -l，平均负载高于 CPU 数量 70% 的时候应该排查问题了 平均负载和CPU使用率平均负载：不仅包括了正在使用 CPU 的进程，还包括等待 CPU 和等待 I/O 的进程 I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高； 工具： stress：压力测试 stress –cpu 1 –timeout 600 模拟1个CPU100% stress -i 1 –timeout 600 模拟 I/O 压力，即不停地执行 sync stress -c 8 –timeout 600 模拟8 个进程： sysstat mpstat：CPU分析工具 mpstat -P ALL 5 1 显示所有 CPU 的指标，并在间隔 5 秒输出一组数据 pidstat：进程分析工具 pidstat -u 5 1 隔 5 秒后输出一组进程数据 进程的io情况，比如可以试试pidstat -d","categories":[{"name":"Linux性能优化","slug":"Linux性能优化","permalink":"https://jiac3366.github.io/categories/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"}],"tags":[]},{"title":"操作系统 | 进程间通信的方式有哪些？","slug":"操作系统/进程间通信方式","date":"2021-09-15T14:56:09.563Z","updated":"2021-12-19T15:56:25.021Z","comments":true,"path":"2021/09/15/操作系统/进程间通信方式/","link":"","permalink":"https://jiac3366.github.io/2021/09/15/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/","excerpt":"","text":"管道 通信⽅式效率低，不适合进程间频繁地交换数据，匿名管道跟随进程的生命周期在 shell ⾥⾯执⾏ A | B 命令的时候，A 进程和 B 进程都是 shell 创建出来的⼦进程如果要进程双向通信，要创建2个管道（不然父进程fork子进程也把管道读写描述符复制了，2者对同一个管道写读造成混乱）​进程写⼊读取的数据都经过内核 | 匿名管道 特殊文件 在内存 mkfifo myPipe 创建命名管道 ls在文件系统能看到文件类型是p(pipe) 消息队列 相比管道，可以频繁交换数据​消息队列跟随内核的生命周期，会有两个宏定义 MSGMAX 和 MSGMNB ，它们以字节为单位，分别定义了⼀条消息的最⼤⻓度和⼀个队列的最⼤⻓度 缺点：⼀是通信不及时，二不适合⽐较⼤数据的传输，三存在⽤户态与内核态之间的数据拷⻉开销 共享内存拿出⼀块虚拟地址空间来，映射到相同的物理内存中 信号量(p191) 防⽌多进程竞争共享资源造成的数据错乱的保护机制，主要⽤于实现进程间的互斥与同步，⽽不是⽤于缓存进程间通信的数据。 信号初始化为 1 ，是互斥信号量在任何时刻只有⼀个进程在访问 信号初始化为 10，是同步信号量保证进程 A 应在进程 B 之前执⾏ 信号 进程间通信机制中唯⼀的异步通信机制 Ctrl+C 产⽣ SIGINT 信号，表示终⽌该进程 Ctrl+Z 产⽣ SIGTSTP 信号，表示停⽌该进程，但还未结束 SocketSocket 通信不仅可以跨⽹络与不同主机的进程间通信，还可以在同主机上进程间通信 UDP 是没有连接的，不需要调⽤ listen 和 connect，要bind绑定IP和端口 因为一台机器上有多网卡，只有绑定IP和端口内核在网卡收到包时才发给我们 socket类型: 6种（协议族：本机/Ipv4/Ipv6，通信特性：字节流/数据报）本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端⼝，⽽是绑定⼀个本地⽂件，这也就是它们之间的最⼤区别。 服务端 socket() –&gt;bind() –&gt;listen() –&gt; accept() ; 客户端 connect() socket() 创建的socket是放在内核的TCP半连接队列吗 listen() 这时候netstat可以查看 accept() 从内核中的 TCP 全连接队列⾥拿出⼀个已经完成连接的 Socket 返回应⽤程序（如果不为空），后续数据传输都⽤这个 Socket","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://jiac3366.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[]},{"title":"后端架构 | 后端存储必知必会","slug":"后端架构/后端存储知识","date":"2021-08-14T10:09:16.833Z","updated":"2021-12-19T16:02:26.621Z","comments":true,"path":"2021/08/14/后端架构/后端存储知识/","link":"","permalink":"https://jiac3366.github.io/2021/08/14/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84/%E5%90%8E%E7%AB%AF%E5%AD%98%E5%82%A8%E7%9F%A5%E8%AF%86/","excerpt":"","text":"CAP理论 不关注单节点系统，也不关注没有数据共享的多节点系统 一致性 保证可用性AP 可用性 保证一致性 分区容忍性（一定需要） 事务的ACID特性（AD必须要保证） A 原子性 C 一致性 I 隔离性 D 持久性C 有流水就有余额变化I 没提交的事务对于其他事务不可见 实现：（不可能100%实现ACID） 数据库事务——四个隔离级别 区分RC(读提交)和RR(可重复读)——一个事务能否读到其他事务对数据已提交的更新 能： RC、不能：RR（mysql默认） 幻读很少遇到也基本不会影响数据准确性RR屏蔽掉了其他事务对真实数据的修改，自己无法查询，但实际上数据已存在或已经修改 兼顾【性能、并发、一致性】的交易方案： 1、给账户余额表增加一个 log_id 属性，记录最后一笔交易的流水号 2、首先开启事务，查询并记录当前账户的余额和最后一笔交易的流水号 3、然后写入流水记录 4、再更新账户余额，但有条件限定：只有流水号等于之前查询出的流水号时才更新 5、然后检查更新余额的返回值，如果更新成功就提交事务，否则回滚事务。 分布式事务——2PC、3PC、TCC、Saga 和本地消息表 下面例子展示不同模型之间的事务交互​分布式事务的“分布式”可以是多主机的系统的事务，也可以是在单机中的不同系统（模型） 实现1：2PC（二阶段提交） 例如:强一致性&amp;并发低场景，订单表（订单系统，下单并绑定消费券id）和消费券表（促销系统，对消费券改为已使用）​2PC 引入了一个事务协调者的角色，来协调两个系统的数据更新操作保持一致，要么都更新成功，要么都更新失败 1、准备阶段：协调者叫各方做准备工作（除了事务提交的所有工作） 2、提交阶段：收到各方”待命”通知后，统一发号令: “提交” 异常处理: 在准备阶段，如果任何一步出现错误或者是超时，协调者就会给两个系统发送“回滚事务”请求 如果已进入提交阶段，整个分布式事务只能成功，反复重试，直到提交成功。如果这个阶段发生宕机，包括两个数据库宕机或者订单服务、促销服务所在的节点宕机，还是有可能出现订单库完成了提交，但促销库因为宕机自动回滚，导致数据不一致的情况。 ps: 2PC 这个协调服务最好和订单服务或者优惠券服务放在同一个进程里，进程更少故障点更少性能更好 缺点：执行过程会阻塞服务端的线程和数据库的会话，协调者是一个单点，宕机会导致事务超时 实现2：本地消息表(可以存MQ/DB/File) 例如:适用在没依赖其他资源(eg:下单不需要锁库存) 实时性要求不高场景，订单表和购物车表 1、在执行这个数据库事务过程中，在本地记录一条消息。这个消息就是一个日志，内容就是“清空购物车”这个操作。我们可以让订单库的事务，来保证记录本地消息和订单库的一致性。完成后可以给客户端返回成功响应 2、用一个异步的服务，读取刚刚记录的清空购物车的本地消息，调用购物车系统的服务清空购物车。购物车清空之后，把本地消息的状态更新成已完成就可以了。异步清空购物车这个过程中，如果操作失败了，可以通过重试来解决。 高并发应对方法（并发高-&gt;分库） 避免重复请求、解决ABA（幂等性问题：其任意多次执行所产生的影响均与一次执行的影响相同） 重复请求：后端提供生成订单号（返回值就是新的，全局唯一的）的服务，订单号作为订单表的主键，前端每次提交就自动带上 ABA：加入版本号字段 Redis保护DB 了解常见缓存策略 Read/Write Through Cache Aside （更好，写操作与Read/Write Through不同） 分库 读写分离 读多写少 与用户关联不大的数据，可以用Redis挡，但是涉及到用户订单等关联大的数据呢？主节点读写兼顾 从节点只读做热备 主从同步问题 主库提交事务 从库复制（mysql半同步复制）mysql5.7可以配置至少几个从节点复制后就返回相应​也可以配置提交事务和复制的先后顺序​默认是先等待复制，再提交事务（同步复制）AFTER_SYNC异步复制有可能丢数据：从节点在复制中主节点挂了 多从库的SLB和高可用方案：HAProxy+KeepAlived 主从延迟问题 主库数据更新后是否需要立刻查询？是：把更新后的查合并成一个微服务放在一个事务中(？)，强制更新后的查打在主库否：更新后几秒后再返回 需要把‘更新、查询’合并成一个事务，所以更新后的查询会被路由到主库 不需要更新后过几秒返回–&gt;下单后等几秒​ 实现： 纯手工方式 组件方式（推荐）得看编程语言的是否有读写分离组件 代理方式（主流）不方便修改应用的代码的情况下 数据量大的应对方法（数据量大-&gt;分表） “拆”——分出历史表 归档历史数据，分出历史订单表如何删除3个月前的订单？详看14 最后一招——用“分片”分表“分片”：通过某字段找到数据在哪个库哪个表eg:在订单号的后几位加入用户ID 就可以根据用户查出订单在哪个库哪个表eg:店铺订单,可以复制一个订单DB，把店铺ID设为Sharding ID​根据什么属性分片就要根据什么属性查，所以对查询会作很多限制 分片算法 时间范围分片易出现热点问题，适用于数据多并发低的系统 订单表hash分片取ID与分片数的模 一致性hash算法hash算法都是为了均匀分布数据 查表法eg:数据可视化系统:商品名映射到某个表 Redis Cluster 存储海量数据 低成本中小集群 如何存？ 数据key与16384取模，余数为所在槽 通过节点的映射表找真正节点在每个节点存一个槽与节点的映射关系 如何保持高可用？ 对每个分片增加从节点，做主从复制从节点一般是热备 也可以读写分离 MySQL to Redis Kafka/RocketMQ Binlog实时更新Redis(更通用)实时解析Binlog –&gt;开源项目：Canal 如何降级或补偿应对数据出现不一致 跨数据系统实时同步 详见笔记p19 MQ多分区存储binlog因果关系的binlog需要hash到同1分区 对面系统的同步程序多线程消费MQ 海量数据存储方案 分布式存储系统：对象存储、HDFS 点击流、监控日志数据 Kafka、HDFSkafka高吞吐 不无限存 查询能力较差​HDFS查询能力好 无限存储 分布式流Pulsar/Bookkeeper、时序数据 (监控数据)InfluxDB/OpenTsDB 海量数据提高查询速度 GB级 给分析系统单独配MySQL 10GB HBase Cassandra ClickHouse 列式DB ES 成本高 内存占用大 但推荐 TB级 定期聚合计算好存在HDFS 再配合Map-Reduce、Spark、Hive做Data聚合计算","categories":[{"name":"后端架构","slug":"后端架构","permalink":"https://jiac3366.github.io/categories/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84/"}],"tags":[]},{"title":"数据库 | SQL知识个人笔记","slug":"数据库/SQL必知必会","date":"2021-08-14T09:41:44.577Z","updated":"2021-12-19T16:30:57.014Z","comments":true,"path":"2021/08/14/数据库/SQL必知必会/","link":"","permalink":"https://jiac3366.github.io/2021/08/14/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/","excerpt":"","text":"Hash索引 hash索引不能用于Order/group by，不能支持最左前缀原则（多个key一起计算hash），不支持模糊查询 适用key-value场景，当mysql某个条件查询频繁，就给这个条件字段生成自适应索引 适用索引的场景 Where字段/value唯一的字段/Distinct字段/Join连接字段 同时进行Group A和Order B的查询时，应使用对应顺序的(A,B)联合索引 不适用索引的场景 数据太少 数据重复太多 数据更新频繁 索引失效场景 字段进行表达式/函数计算 WHERE中使用OR的2个字段没有都建立索引 LIKE 直接跟% 索引列与NULL进行判断(所以要把字段设为非空) (A,B)的联合索引用了B查询 从磁盘I/O角度理解SQL查询的成本 查看缓冲池的大小，当缓冲池大小&gt;1G时，缓冲池的实例数才可以修改[修改缓冲池大小：set global innodb_buffer_pool_size = 134217728] 数据页加载的3种方式 查询速度：缓冲池&gt;内存&gt;磁盘 内存读取（1次读取1条记录 1ms） 随机读取（在磁盘找页，10ms，6ms等磁盘，3ms排队，1ms传输） 顺序读取（批量读取）存储介质物理特性：顺序读&gt;多次随机读，设磁盘吞吐量40MB/s，40M/16kB = 0.4ms，每秒读约2560页​ 统计SQL查询成本，可以看到上一条SQL要读取页的数量[SHOW STATUS LIKE ‘last_query_cost’] 关于锁 共享锁（S锁）–&gt;只读，排他锁（X锁）–&gt;读写均不可操作 表级锁 解锁：UNLOCK TABLE; 表锁：共享锁 LOCK TABLE product_comment READ; 排他锁 LOCK TABLE product_comment WRITE; MDL：MDL（MetaData Lock）就是针对于 DDL 与 DML、DQL 操作加锁，执行 DDL 自动添加写锁，执行 DML、DQL 自动添加读锁，也就是说 DML 语句可以同时执行（不考虑其他锁），而 DDL 间则会相互阻塞深入理解MYSQL的MDL元数据锁 - zengkefu - 博客园 (cnblogs.com) 数据行 共享锁 SELECT xx FROM table WHERE user_id = 912178 LOCK IN SHARE MODE 排他锁 SELECT xx FROM table WHERE user_id = 912178 FOR UPDATE 意向共享/排他锁：当事务操作某些记录的数据，会在表上添加对应的意向锁，提示其他事务有人”占”了表中的某些记录 因为共享锁允许其他事务加共享锁(共享锁不排他)，多个读锁可能出现死锁当第二个事务使用UPDATE等操作加上排他锁的操作时，会不停地等待，超时 乐观锁和悲观锁 是程序员对待数据并发风险的一种态度体现 乐观锁（认为别人不会同时写入），使用版本机制或时间戳实现控制同一数据被“同时修改”版本机制：后端存储知识 中的【区分 RR(可重复读)/RC(读提交)】小节的解决方案 悲观，用数据量自身的锁机制 死锁 因为在事务（进程）中，锁的获取是逐步的 有锁的存在，死锁就有可能， eg:2个事务都对资源获取共享锁，结果都没法更新操作 如何避免死锁？？？ 若事务涉及多个表或一个表的大部分数据时，可以一次性锁定多个表或整个表 不同事务并发多张表，可以约定它们访问表的顺序(? 死锁检测（并发降低） 如何解决死锁 临时关闭死锁检测 控制并发度 添加超时时间 事务隔离如何实现？ MVCC可以解决什么问题的？ 优点： 通过版本号解决数据是否显示，即使读不加锁也实现数据隔离 读写互不阻塞，并发提高 解决一致性读（快照读）问题，查数据时只能读到这个时间点前事务提交的结果 快照读/当前读 快照读：不加锁的select 当前读：显示加锁的操作 MVCC = Undo log(mv) + ReadView(cc) Undo log操作这个数据的事务ID 遍历行记录的回滚指针可以查历史快照 ReadView RR与RC在读取ReadView中存在不同 RC每次读都会获取一次ReadView RR会复用第一次读的ReadView所以见不到后面事务提交的数据 InnoDB如何解决幻读 在RR情况下，InnoDB通过Next Key + MVCC解决 行锁种类 记录锁（RC采用），锁记录 间隙锁，锁记录旁，索引与索引之间的空隙 Next Key，等于记录锁+间隙锁 RR级别业务中解决幻读的方案 单数据库事务：参考版本机制：后端存储知识 中的【区分 RR(可重复读)/RC(读提交)】小节的解决方案 多数据库：引入流水功能，流水本身不要出现数据一致性问题，所以字段要包括双方的帐户字段 王争设计模式给出的流水方案–保持数据一致性 查询优化器如何工作 逻辑优化——查询重写属于代数级语法级的优化 物理查询优化——基于代价的估算模型从连接路径中选代价最小的路径 CBO RBO 使用性能分析工具定位SQL执行慢的原因 …","categories":[{"name":"数据库","slug":"数据库","permalink":"https://jiac3366.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[]}],"categories":[{"name":"网络","slug":"网络","permalink":"https://jiac3366.github.io/categories/%E7%BD%91%E7%BB%9C/"},{"name":"数据库","slug":"数据库","permalink":"https://jiac3366.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Python高级","slug":"Python高级","permalink":"https://jiac3366.github.io/categories/Python%E9%AB%98%E7%BA%A7/"},{"name":"Golang","slug":"Golang","permalink":"https://jiac3366.github.io/categories/Golang/"},{"name":"后端架构","slug":"后端架构","permalink":"https://jiac3366.github.io/categories/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://jiac3366.github.io/categories/Kubernetes/"},{"name":"容器技术","slug":"容器技术","permalink":"https://jiac3366.github.io/categories/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"},{"name":"Linux性能优化","slug":"Linux性能优化","permalink":"https://jiac3366.github.io/categories/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"操作系统","slug":"操作系统","permalink":"https://jiac3366.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[]}